{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYpDFySzrk4a"
      },
      "source": [
        "# Seminar on Graphs for NLP: Vector representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlhZKxv4rs2F"
      },
      "source": [
        "## Plan for today:\n",
        "\n",
        "#### 0. What a taxonomy is. Taxonomy Enrichment task.\n",
        "#### 1. Node2vec model. Implementation of node2vec.\n",
        "#### 2. Embedding generation for the OOV words for node2vec. Linear transformation model.\n",
        "#### 3. Graph Neural networks: GCN and GAT\n",
        "#### 4. GraphBERT: Only Attention is Needed for Learning Graph Representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z0GruYny1kG"
      },
      "source": [
        "# 0. Taxonomy\n",
        "\n",
        "A taxonomy is a hierarchical structure of units in terms if class inclusion such that superordinate units in the hierarchy include, or subsume, all items in subordinate units. Taxonomies are typically represented as having tree structures.\n",
        "\n",
        "![](https://www.digital-mr.com/media/cache/51/6f/516f493d37a7b4895f678843b6383e48.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxGSENmE6IsK"
      },
      "source": [
        "Taxonomies can be represented as graphs!\n",
        "\n",
        "Let us download the most popular and well-known taxonomy called WordNet. You may also use the `from nltk.corpus import wordnet as wn`, but keep in mind that you can operate with earlier versions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-hshM6rL9z",
        "outputId": "0b06dbe3-43d0-462c-cb20-84959743b618"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 35.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 41.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C91pcHIwsg_v",
        "outputId": "7a171832-21d1-433d-88d1-bae889c45275"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 16.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "V5VOiYm_2jjR",
        "outputId": "8bc95902-58c5-45e7-879a-a75cb82b176d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1avRebH3BMsolRxmthVFNPoLwyRpAV2tx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtnbZKRXvVhk",
        "outputId": "4858de35-4f9d-444b-f5c5-ef06e303280f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1avRebH3BMsolRxmthVFNPoLwyRpAV2tx\n",
            "To: /content/wordnet_n_is_directed_1_en_synsets.zip\n",
            "100% 217M/217M [00:04<00:00, 46.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip wordnet_n_is_directed_1_en_synsets.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORTr2AWjwIvu",
        "outputId": "8f59cb41-1ae7-4c0b-c88b-fd284108753e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wordnet_n_is_directed_1_en_synsets.zip\n",
            "   creating: wordnet_n_is_directed_1_en_synsets/\n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/link  \n",
            "   creating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/\n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/.ipynb_checkpoints/link-checkpoint  \n",
            "  inflating: wordnet_n_is_directed_1_en_synsets/node  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTwSPeZcjmZT",
        "outputId": "bf3d1221-218a-492a-fd28-10b5671ce0db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.poincare import PoincareModel\n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "kX-L6-NLeKJ8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "Ap6gnMUfy146"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hiwh7Xhcy9hm",
        "outputId": "6d4c2e77-7ce2-473b-c51a-0b36c442fb74"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"guy.n.01\").lemmas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40OqktlGy5Wd",
        "outputId": "523b337a-4cba-463d-de0e-64df659b7f93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('guy.n.01.guy'),\n",
              " Lemma('guy.n.01.cat'),\n",
              " Lemma('guy.n.01.hombre'),\n",
              " Lemma('guy.n.01.bozo')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = f\"wordnet_n_is_directed_1_en_synsets/\"\n",
        "\n",
        "link_path = os.path.join(path, \"link\")\n",
        "node_path = os.path.join(path, \"node\")"
      ],
      "metadata": {
        "id": "XeGdLaEqekbb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2synset = {}\n",
        "fasttext_dict = {}\n",
        "\n",
        "with open(node_path) as f:\n",
        "    for line in f:\n",
        "        line_split = line.split(\"\\t\")\n",
        "        id2synset[line_split[0].strip()] = line_split[-1].strip()\n",
        "        fasttext_dict[line_split[-1].strip()] = np.array([float(num) for num in line_split[1:-1]])"
      ],
      "metadata": {
        "id": "C26HaOfGfDkR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link_pairs = set()\n",
        "with open(link_path) as f:\n",
        "    for line in f:\n",
        "        line_split = line.split(\"\\t\")\n",
        "        link_pairs.add((id2synset[line_split[0].strip()], id2synset[line_split[-1].strip()]))"
      ],
      "metadata": {
        "id": "i7hmkbEXfFIG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Arh9U60uDH8"
      },
      "source": [
        "# 4. Graph Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "46UD5px_rcNb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils"
      ],
      "metadata": {
        "id": "B1qcGlrbre_y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eM2IXSZMuIq8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.utils import train_test_split_edges\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPMU0mL6IJEA"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2NAxJKWLmkae"
      },
      "outputs": [],
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "fasttext = KeyedVectors(vector_size=300)\n",
        "fasttext.add_vectors(list(fasttext_dict.keys()), list(fasttext_dict.values()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx"
      ],
      "metadata": {
        "id": "JL1yQgaD4mio"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = nx.DiGraph()\n",
        "\n",
        "for pair in link_pairs:\n",
        "    G.add_edge(*pair)"
      ],
      "metadata": {
        "id": "WkLOHE1t4qJo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "cgC4chwuIJEC"
      },
      "outputs": [],
      "source": [
        "def create_edge_list(G):\n",
        "    starts = []\n",
        "    ends = []\n",
        "    for left, right in G.edges:\n",
        "        if left in fasttext.key_to_index and right in fasttext.key_to_index:\n",
        "            starts.append(fasttext.key_to_index[left])\n",
        "            ends.append(fasttext.key_to_index[right])\n",
        "    return torch.tensor([starts, ends], dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ILgb5IF4a2A9"
      },
      "outputs": [],
      "source": [
        "index_to_key = dict(map(reversed, fasttext.key_to_index.items()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "f_iX7WH2nCSv"
      },
      "outputs": [],
      "source": [
        "edge_index = create_edge_list(G)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([fasttext[index_to_key[int(i)]] for i in index_to_key], dtype=torch.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZJdPogQy1kG",
        "outputId": "2911fe10-0b6c-4789-fcdd-e19f05992e22"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poQi6NZorQfp",
        "outputId": "4e637b48-350f-4f9a-e545-8211adfa6e9f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([78748, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data(x=x, edge_index=edge_index)\n",
        "#data = train_test_split_edges(data)"
      ],
      "metadata": {
        "id": "WC3VgJ0FyiiH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.transforms import RandomLinkSplit"
      ],
      "metadata": {
        "id": "TCVO4jfK1LFI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = RandomLinkSplit(is_undirected=True, split_labels=True)\n",
        "train_data, val_data, test_data = transform(data)"
      ],
      "metadata": {
        "id": "lPA8jPvZ1IIn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCZE_0e0IJED"
      },
      "source": [
        "### GCN and GAT Encoder\n",
        "\n",
        "The following code snippet describes the Encoder module with GCN or GAT networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I-naFqNRumvk"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mode=\"gcn\"):\n",
        "        super(Encoder, self).__init__()\n",
        "        if mode == \"gcn\":\n",
        "            self.conv1 = pyg_nn.GCNConv(in_channels, 2 * out_channels, cached=True)\n",
        "            self.conv2 = pyg_nn.GCNConv(2 * out_channels, out_channels, cached=True)\n",
        "        elif mode == 'gat':\n",
        "            self.conv1 = pyg_nn.GATConv(in_channels, 2 * out_channels)\n",
        "            self.conv2 = pyg_nn.GATConv(2 * out_channels, out_channels)\n",
        "        else:\n",
        "            raise Exception(\"Encoder mode is not recognized, try gcn/gat\")\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        return self.conv2(x, edge_index)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(x, train_pos_edge_index)\n",
        "    loss = model.recon_loss(z, train_pos_edge_index)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    writer.add_scalar(\"loss\", loss.item(), epoch)\n",
        "    return loss.item()\n",
        "\n",
        "def test(pos_edge_index, neg_edge_index):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, train_pos_edge_index)\n",
        "    return model.test(z, pos_edge_index, neg_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eplyGdMwvF57",
        "outputId": "605dbadc-4b2f-403b-b723-cc1e95fd9bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA availability: True\n"
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "channels = 64\n",
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('CUDA availability:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHEHYEbcXCFG"
      },
      "source": [
        "## Variational Graph Auto-Encoders\n",
        "\n",
        "https://arxiv.org/pdf/1611.07308.pdf\n",
        "\n",
        "The pipeline is working as follows: first, we train a graph autoencoder with GCN or GAT under the hoot. During the evaluation phase, the latent representations of the autoencoder are actually the embeddings we are looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mqZGTsQ7vRQM",
        "outputId": "b933a2f1-8057-4b1a-fc79-e2f2a49f8b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 010, AUC: 0.8235, AP: 0.8087, Loss: 0.9674\n",
            "Epoch: 020, AUC: 0.8606, AP: 0.8509, Loss: 0.8795\n",
            "Epoch: 030, AUC: 0.8839, AP: 0.8770, Loss: 0.8350\n",
            "Epoch: 040, AUC: 0.8899, AP: 0.8866, Loss: 0.8199\n",
            "Epoch: 050, AUC: 0.8975, AP: 0.8956, Loss: 0.8090\n",
            "Epoch: 060, AUC: 0.9017, AP: 0.9012, Loss: 0.7974\n",
            "Epoch: 070, AUC: 0.9057, AP: 0.9064, Loss: 0.7931\n",
            "Epoch: 080, AUC: 0.9069, AP: 0.9091, Loss: 0.7868\n",
            "Epoch: 090, AUC: 0.9095, AP: 0.9122, Loss: 0.7862\n",
            "Epoch: 100, AUC: 0.9112, AP: 0.9142, Loss: 0.7817\n",
            "Epoch: 110, AUC: 0.9111, AP: 0.9152, Loss: 0.7829\n",
            "Epoch: 120, AUC: 0.9125, AP: 0.9167, Loss: 0.7790\n",
            "Epoch: 130, AUC: 0.9124, AP: 0.9172, Loss: 0.7727\n",
            "Epoch: 140, AUC: 0.9133, AP: 0.9184, Loss: 0.7729\n",
            "Epoch: 150, AUC: 0.9136, AP: 0.9189, Loss: 0.7673\n",
            "Epoch: 160, AUC: 0.9135, AP: 0.9192, Loss: 0.7675\n",
            "Epoch: 170, AUC: 0.9153, AP: 0.9210, Loss: 0.7674\n",
            "Epoch: 180, AUC: 0.9150, AP: 0.9213, Loss: 0.7641\n",
            "Epoch: 190, AUC: 0.9148, AP: 0.9213, Loss: 0.7646\n",
            "Epoch: 200, AUC: 0.9142, AP: 0.9209, Loss: 0.7619\n",
            "Epoch: 210, AUC: 0.9151, AP: 0.9223, Loss: 0.7638\n",
            "Epoch: 220, AUC: 0.9166, AP: 0.9237, Loss: 0.7619\n",
            "Epoch: 230, AUC: 0.9159, AP: 0.9233, Loss: 0.7581\n",
            "Epoch: 240, AUC: 0.9152, AP: 0.9227, Loss: 0.7621\n",
            "Epoch: 250, AUC: 0.9149, AP: 0.9226, Loss: 0.7613\n",
            "Epoch: 260, AUC: 0.9139, AP: 0.9219, Loss: 0.7601\n",
            "Epoch: 270, AUC: 0.9147, AP: 0.9224, Loss: 0.7575\n",
            "Epoch: 280, AUC: 0.9145, AP: 0.9226, Loss: 0.7565\n",
            "Epoch: 290, AUC: 0.9154, AP: 0.9232, Loss: 0.7557\n",
            "Epoch: 300, AUC: 0.9159, AP: 0.9234, Loss: 0.7534\n",
            "Epoch: 310, AUC: 0.9150, AP: 0.9231, Loss: 0.7573\n",
            "Epoch: 320, AUC: 0.9159, AP: 0.9239, Loss: 0.7582\n",
            "Epoch: 330, AUC: 0.9152, AP: 0.9234, Loss: 0.7546\n",
            "Epoch: 340, AUC: 0.9149, AP: 0.9234, Loss: 0.7533\n",
            "Epoch: 350, AUC: 0.9150, AP: 0.9234, Loss: 0.7523\n",
            "Epoch: 360, AUC: 0.9155, AP: 0.9238, Loss: 0.7535\n",
            "Epoch: 370, AUC: 0.9155, AP: 0.9240, Loss: 0.7540\n",
            "Epoch: 380, AUC: 0.9151, AP: 0.9238, Loss: 0.7511\n",
            "Epoch: 390, AUC: 0.9137, AP: 0.9230, Loss: 0.7511\n",
            "Epoch: 400, AUC: 0.9142, AP: 0.9236, Loss: 0.7507\n"
          ]
        }
      ],
      "source": [
        "model = pyg_nn.GAE(Encoder(300, channels, 'gcn')).to(dev)\n",
        "x, train_pos_edge_index = train_data.x.to(dev), train_data.pos_edge_label_index.to(dev)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(1, 401):\n",
        "    loss = train(epoch)\n",
        "    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)\n",
        "    writer.add_scalar(\"AUC\", auc, epoch)\n",
        "    writer.add_scalar(\"AP\", ap, epoch)\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}, Loss: {:.4f}'.format(epoch, auc, ap, loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3__lKOCOXNKj"
      },
      "source": [
        "#### Examples\n",
        "\n",
        "Let us see the nearest neighbours for the unseen words from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1mWcc2HlvVLf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "new_x = torch.tensor([fasttext[index_to_key[i]] for i in index_to_key], dtype=torch.float).to(dev)\n",
        "z = model.encode(new_x, train_pos_edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2syns = {}\n",
        "syns2id = {}\n",
        "with open('wordnet_n_is_directed_1_en_synsets/node') as f:\n",
        "    for line in f:\n",
        "        id2syns[line.split()[0]] = line.split()[-1]\n",
        "        syns2id[line.split()[-1]] = line.split()[0]"
      ],
      "metadata": {
        "id": "HoDN-uTq8bem"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "par2orph = {}\n",
        "orph2par = {}\n",
        "with open('wordnet_n_is_directed_1_en_synsets/link') as f:\n",
        "    for line in f:\n",
        "        par_id = line.split()[0]\n",
        "        child_id = line.split()[-1]\n",
        "        \n",
        "        if \"ORPHAN_\" in id2syns[child_id]:\n",
        "            par2orph[id2syns[par_id]] = id2syns[child_id]\n",
        "            orph2par[id2syns[child_id]] = id2syns[par_id]"
      ],
      "metadata": {
        "id": "DGWVX-WL74KV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for word in fasttext.key_to_index:\n",
        "    if \".n.\" not in word:\n",
        "        cur_index = fasttext.key_to_index[word]\n",
        "        tensor_ = torch.tensor([[cur_index]*(len(G.nodes)), [i for i in range(0, len(G.nodes))]])\n",
        "        results = model.decode(z, tensor_)\n",
        "        top10 = list(reversed(sorted([(index_to_key[i], round(float(score.cpu().detach().float()), 4)) for i, score in enumerate(results)], key=lambda x: x[1])))[:10]       \n",
        "        print(orph2par[word], \":\", top10)\n",
        "        print(\"=\"*10)\n",
        "        c += 1\n",
        "        if c == 20:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4jW1K0d3Fq1",
        "outputId": "d17459e1-8d23-4ab9-aa08-2f9a92102e87"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "course.n.04 : [('way.n.05', 0.9996), ('ORPHAN_100000000', 0.9995), ('golf_course.n.01', 0.9993), ('appetizer.n.01', 0.9986), ('course.n.04', 0.9983), ('dessert.n.01', 0.9981), ('course.n.07', 0.9976), ('course.n.09', 0.9976), ('scheme.n.01', 0.9968), ('system.n.07', 0.9961)]\n",
            "==========\n",
            "recovery.n.03 : [('pump_action.n.01', 0.9676), ('movement.n.10', 0.9666), ('transmission.n.01', 0.9653), ('recovery.n.03', 0.9651), ('reclamation.n.02', 0.9643), ('conservation.n.02', 0.9641), ('action.n.01', 0.9632), ('ORPHAN_100000062', 0.9618), ('ORPHAN_100000001', 0.9603), ('quitclaim.n.01', 0.9586)]\n",
            "==========\n",
            "disappearance.n.01 : [('due.n.02', 0.9858), ('ORPHAN_100000002', 0.9849), ('vanishing.n.01', 0.9683), ('cheap_shot.n.02', 0.9667), ('flowage.n.01', 0.96), ('outflow.n.02', 0.9593), ('flood.n.01', 0.9541), ('relict.n.01', 0.9534), ('erasure.n.02', 0.9525), ('inpouring.n.01', 0.9512)]\n",
            "==========\n",
            "hit.n.03 : [('hit.n.07', 0.9921), ('base_hit.n.01', 0.9712), ('hit.n.01', 0.9557), ('collision.n.01', 0.9536), ('cog.n.01', 0.9503), ('hit_parade.n.02', 0.9484), ('debut.n.02', 0.9441), ('break.n.02', 0.9377), ('lottery.n.01', 0.9375), ('slam.n.03', 0.9367)]\n",
            "==========\n",
            "breach.n.01 : [('instrument_of_torture.n.01', 0.9951), ('filling.n.06', 0.9949), ('damage.n.03', 0.9945), ('hardening.n.03', 0.9936), ('ORPHAN_100000034', 0.9931), ('change_of_color.n.01', 0.9929), ('combination.n.07', 0.9928), ('explosion.n.02', 0.9927), ('burning.n.01', 0.9926), ('separation.n.09', 0.9923)]\n",
            "==========\n",
            "buying.n.01 : [('ORPHAN_100000005', 0.9852), ('buying.n.01', 0.9847), ('purchase.n.01', 0.9675), ('redemption.n.03', 0.964), ('durables.n.01', 0.9635), ('product.n.02', 0.9632), ('sharing.n.01', 0.9625), ('entail.n.02', 0.9623), ('tasting.n.01', 0.9621), ('spending.n.01', 0.9616)]\n",
            "==========\n",
            "restitution.n.03 : [('ORPHAN_100000006', 0.9862), ('reserve_assets.n.01', 0.9785), ('investment.n.02', 0.9782), ('liquid_assets.n.01', 0.9775), ('capital.n.02', 0.9773), ('sum.n.01', 0.9769), ('intangible.n.01', 0.9764), ('capital.n.01', 0.975), ('receivables.n.01', 0.9741), ('crown_jewel.n.01', 0.974)]\n",
            "==========\n",
            "abandonment.n.03 : [('unloading.n.01', 0.9602), ('expropriation.n.01', 0.9601), ('reserve_assets.n.01', 0.9589), ('loss.n.05', 0.9586), ('material_resource.n.01', 0.9566), ('loading.n.05', 0.9561), ('ORPHAN_100000007', 0.9561), ('resource.n.01', 0.956), ('intangible.n.01', 0.9559), ('omission.n.04', 0.9545)]\n",
            "==========\n",
            "mine_disposal.n.01 : [('job.n.04', 0.9651), ('job.n.03', 0.9651), ('job.n.05', 0.9651), ('job.n.06', 0.9651), ('salt_mine.n.01', 0.9478), ('scut_work.n.01', 0.9405), ('silver_mine.n.01', 0.9404), ('sea.n.01', 0.9386), ('sulphur_mine.n.01', 0.9366), ('south_sea.n.01', 0.9332)]\n",
            "==========\n",
            "expiation.n.02 : [('ORPHAN_100000009', 0.9937), ('reparation.n.02', 0.9877), ('redemption.n.02', 0.9813), ('derogation.n.02', 0.9806), ('recompense.n.01', 0.9805), ('restoration.n.02', 0.9795), ('revocation.n.02', 0.9793), ('mercy.n.05', 0.9793), ('penalty.n.02', 0.9781), ('sin.n.05', 0.9777)]\n",
            "==========\n",
            "rendition.n.04 : [('representation.n.02', 0.9871), ('invention.n.02', 0.9843), ('product.n.02', 0.9838), ('needlework.n.01', 0.976), ('rendering.n.06', 0.9755), ('rendering.n.07', 0.9755), ('creation.n.02', 0.9713), ('flowage.n.01', 0.9702), ('ORPHAN_100000010', 0.97), ('depicting.n.01', 0.9645)]\n",
            "==========\n",
            "depression.n.10 : [('reset_button.n.01', 0.9395), ('chess_move.n.01', 0.9364), ('push_button.n.01', 0.9152), ('nudge.n.01', 0.907), ('move.n.05', 0.9057), ('shove.n.01', 0.8915), ('tick.n.04', 0.8864), ('push.n.02', 0.8859), ('depression.n.10', 0.8853), ('press.n.09', 0.8814)]\n",
            "==========\n",
            "blink.n.01 : [('notebook_entry.n.01', 0.9048), ('entry.n.01', 0.8774), ('covert.n.01', 0.875), ('cervical_smear.n.01', 0.8423), ('adjusting_entry.n.01', 0.8387), ('biont.n.01', 0.8364), ('ORPHAN_100000485', 0.829), ('drawee.n.01', 0.8265), ('foglamp.n.01', 0.8229), ('ORPHAN_100000012', 0.8215)]\n",
            "==========\n",
            "shooting.n.01 : [('cheap_shot.n.02', 0.9988), ('gunman.n.02', 0.9974), ('shot.n.12', 0.9963), ('shot.n.14', 0.9963), ('trapshooter.n.01', 0.9956), ('shot.n.05', 0.9943), ('pivot_shot.n.01', 0.9877), ('passing_shot.n.01', 0.9855), ('set_shot.n.01', 0.9851), ('headshot.n.02', 0.9846)]\n",
            "==========\n",
            "hit.n.02 : [('hit.n.07', 0.997), ('stroke.n.04', 0.9906), ('finishing_touch.n.01', 0.9881), ('cheap_shot.n.02', 0.9851), ('base_hit.n.01', 0.9822), ('brush.n.03', 0.9817), ('touch.n.11', 0.978), ('hit.n.02', 0.975), ('fly.n.04', 0.9721), ('crash.n.04', 0.972)]\n",
            "==========\n",
            "free_kick.n.01 : [('kick.n.05', 0.9792), ('place_kick.n.01', 0.9717), ('punt.n.03', 0.966), ('run.n.15', 0.962), ('nibble.n.02', 0.9599), ('munch.n.02', 0.9596), ('flutter_kick.n.01', 0.9593), ('goal-kick.n.01', 0.956), ('goal-kick.n.02', 0.956), ('frog_kick.n.01', 0.9515)]\n",
            "==========\n",
            "assignment.n.03 : [('ORPHAN_100000016', 0.9731), ('reallotment.n.01', 0.9693), ('sorting.n.03', 0.9692), ('rationing.n.01', 0.9677), ('overage.n.01', 0.9664), ('loading.n.05', 0.9661), ('motor_control.n.01', 0.9652), ('unloading.n.01', 0.9644), ('parcel.n.02', 0.9637), ('separation.n.04', 0.9629)]\n",
            "==========\n",
            "road.n.02 : [('pathway.n.02', 0.9872), ('towpath.n.01', 0.986), ('roadway.n.01', 0.9829), ('walk.n.05', 0.9816), ('side_road.n.01', 0.9816), ('skid_road.n.02', 0.9762), ('track.n.10', 0.9752), ('path.n.02', 0.9746), ('turnoff.n.02', 0.9726), ('speedway.n.01', 0.9717)]\n",
            "==========\n",
            "vote.n.02 : [('ORPHAN_100000018', 0.9998), ('vote.n.04', 0.9994), ('vote.n.02', 0.9994), ('pocket_veto.n.01', 0.9974), ('ORPHAN_100000385', 0.9957), ('write-in.n.02', 0.9957), ('multiple_voting.n.01', 0.9956), ('secret_ballot.n.01', 0.9945), ('veto.n.01', 0.9937), ('split_ticket.n.01', 0.9934)]\n",
            "==========\n",
            "economy.n.04 : [('ORPHAN_100000037', 0.9794), ('change_of_course.n.01', 0.9793), ('bending.n.03', 0.9792), ('improvement.n.02', 0.9769), ('reversion.n.04', 0.9737), ('change_of_color.n.01', 0.973), ('ORPHAN_100000019', 0.9718), ('loss.n.02', 0.9717), ('separation.n.09', 0.9706), ('filling.n.06', 0.9695)]\n",
            "==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgbmlZqBIJEN"
      },
      "source": [
        "## GraphBERT\n",
        "\n",
        "https://github.com/jwzhanggy/Graph-Bert\n",
        "\n",
        "Yet another model for embedding generation is GraphBert. Instead of feeding large input graph, we train GRAPH-BERT with sampled subgraphs within their local contexts. The input vector embeddings to be fed to the graphtransformer model actually cover four parts: (1) raw feature vector embedding, (2) Weisfeiler-Lehman absolute role embedding, (3) intimacy based relative positional embedding, and (4) hop based relative distance embedding, respectively.\n",
        "\n",
        "GRAPH-BERT is trained with the node attribute reconstruction and structure recovery tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo0z3trCa2BC"
      },
      "source": [
        "![](https://github.com/jwzhanggy/Graph-Bert/raw/master/result/screenshot/model.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fOqFVHLa2BD"
      },
      "source": [
        "## Subgraph Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5htpR5J7a2BD"
      },
      "source": [
        "![](https://i.ibb.co/5cbjJZ6/photo-2021-12-07-16-41-32.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDsdGrffa2BD"
      },
      "source": [
        "## Positional embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOny-pLQa2BE"
      },
      "source": [
        "### Weisfeiler-Lehman Absolute Role Embedding\n",
        "\n",
        "![](https://i.ibb.co/bgT7gqb/wl.png)\n",
        "\n",
        "### Intimacy based Relative Positional Embedding\n",
        "\n",
        "![](https://i.ibb.co/34FvCf0/photo-2021-12-07-16-52-30.jpg)\n",
        "\n",
        "### Hop based Relative Distance Embedding\n",
        "![](https://i.ibb.co/tCzRcfK/hops-drawio.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AomlgEcXa2BE"
      },
      "source": [
        "Actually, you are simply expected to run two scripts: `script_1_preprocess.py` and `script_2_pre_train.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DXA9H5la2BF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7713f6b9-f95c-4f0e-8d7f-43a7d970f271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Graph-Bert'...\n",
            "remote: Enumerating objects: 442, done.\u001b[K\n",
            "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 442 (delta 102), reused 78 (delta 78), pack-reused 314\u001b[K\n",
            "Receiving objects: 100% (442/442), 2.23 MiB | 21.11 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jwzhanggy/Graph-Bert.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueo45TqXa2BF",
        "outputId": "2afa1c6e-1d95-45aa-ba5f-f5520396360d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Graph-Bert'\n",
            "/home/nikishina/Graph-Bert\n",
            "************ Start ************\n",
            "WL, dataset: wordnet_n_is_directed_1_en_synsets_2.0\n",
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "Subgraph Batching, dataset: wordnet_n_is_directed_1_en_synsets_2.0, k: 5\n",
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "/home/nikishina/Graph-Bert/code/DatasetLoader.py:63: RuntimeWarning: divide by zero encountered in power\n",
            "  r_inv = np.power(rowsum, -0.5).flatten()\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "HopDistance, dataset: wordnet_n_is_directed_1_en_synsets_2.0, k: 5\n",
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "************ Finish ************\n"
          ]
        }
      ],
      "source": [
        "%cd Graph-Bert\n",
        "!python3 script_1_preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkH0wplPa2BG",
        "outputId": "6d277591-4b03-440d-eea9-9b162028fd0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "************ Start ************\n",
            "GrapBert, dataset: wordnet_n_is_directed_1_en_synsets_2.0, Pre-training, Node Attribute Reconstruction.\n",
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "Load WL Dictionary\n",
            "Load Hop Distance Dictionary\n",
            "Load Subgraph Batches\n",
            "Epoch: 0001 loss_train: 0.0067 time: 0.3729s\n",
            "Epoch: 0051 loss_train: 0.0027 time: 0.1480s\n",
            "Epoch: 0101 loss_train: 0.0026 time: 0.3395s\n",
            "Epoch: 0151 loss_train: 0.0025 time: 0.3424s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 67.0946s\n",
            "Save pretrained model in ./result/PreTrained_GraphBert/wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model/\n",
            "************ Finish ************\n",
            "************ Start ************\n",
            "GrapBert, dataset: wordnet_n_is_directed_1_en_synsets_2.0, Pre-training, Graph Structure Recovery.\n",
            "Load pretrained model from ./result/PreTrained_GraphBert/wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model/\n",
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "Load WL Dictionary\n",
            "Load Hop Distance Dictionary\n",
            "Load Subgraph Batches\n",
            "Epoch: 0001 loss_train: 0.2364 time: 55.3349s\n",
            "Epoch: 0051 loss_train: 0.0338 time: 45.0894s\n",
            "Epoch: 0101 loss_train: 0.0325 time: 43.0157s\n",
            "Epoch: 0151 loss_train: 0.0323 time: 40.1319s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 9287.4034s\n",
            "Save pretrained model in ./result/PreTrained_GraphBert/wordnet_n_is_directed_1_en_synsets_2.0/node_graph_reconstruct_model/\n",
            "************ Finish ************\n"
          ]
        }
      ],
      "source": [
        "!python3 script_2_pre_train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYtahIlwa2BG"
      },
      "source": [
        "After the model has been trained, we predict embeddings for the new (unseen words) and their nearest neighbours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgurRJ-Ca2BH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "sys.path.append(\"/home/nikishina/Graph-Bert/code\")\n",
        "sys.path.append(\"/home/nikishina/Graph-Bert/\")\n",
        "from DatasetLoader import DatasetLoader\n",
        "from MethodBertComp import GraphBertConfig\n",
        "from MethodGraphBertGraphRecovery import MethodGraphBertGraphRecovery\n",
        "from MethodGraphBertNodeConstruct import MethodGraphBertNodeConstruct\n",
        "from itertools import combinations\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "\n",
        "\n",
        "def load_data(dataset_path, k, device):\n",
        "    data_obj = DatasetLoader()\n",
        "    data_obj.dataset_source_folder_path = '/home/nikishina/Graph-Bert/data/' + dataset_path + '/'\n",
        "    data_obj.dataset_name = dataset_path\n",
        "    data_obj.k = k\n",
        "    data_obj.device = device\n",
        "    data_obj.load_all_tag = True\n",
        "    return data_obj.load()\n",
        "\n",
        "\n",
        "def get_query_embedding(word, final_embeddings, index_id_map):\n",
        "    offset, definition = wn.synset(word).offset(), wn.synset(word).definition()\n",
        "    index_of_synset = None\n",
        "\n",
        "    for i, j in index_id_map.items():\n",
        "        if j == offset:\n",
        "            index_of_synset = i\n",
        "            break\n",
        "\n",
        "    query_embedding = final_embeddings[index_of_synset]\n",
        "    return query_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbIUXhyia2BH"
      },
      "outputs": [],
      "source": [
        "class GraphBERTEmbeddingsSaver:\n",
        "    def __init__(self, model_name, model, x_size=300, device='cpu', max_index=132, intermediate_size=32,\n",
        "                 num_attention_heads=2, num_hidden_layers=2, y_size=0, residual_type='graph_raw', k=5, nfeature=300):\n",
        "\n",
        "        pretrained_path = './result/PreTrained_GraphBert/' + model_name\n",
        "        bert_config = GraphBertConfig(residual_type=residual_type, k=k, x_size=x_size, y_size=y_size,\n",
        "                                      hidden_size=intermediate_size, intermediate_size=intermediate_size,\n",
        "                                      num_attention_heads=num_attention_heads, num_hidden_layers=num_hidden_layers,\n",
        "                                      max_wl_role_index=max_index, max_hop_dis_index=max_index,\n",
        "                                      max_inti_pos_index=max_index)\n",
        "\n",
        "        self.model = model(bert_config, pretrained_path, device=device)\n",
        "        self.model.eval()\n",
        "        self.nfeature = nfeature\n",
        "\n",
        "    def compute_and_save_embeddings(self, data, test_synsets, index_id_map, id2label, result_dir):\n",
        "        final_embeddings = self.compute_embeddings(data, index_id_map, id2label)\n",
        "        self.save_embeddings(test_synsets, final_embeddings, result_dir)\n",
        "\n",
        "    def compute_embeddings(self, data, index_id_map, id2label):\n",
        "        final_embeddings = np.zeros(shape=(len(index_id_map), self.nfeature), dtype=np.float32)\n",
        "\n",
        "        for _index, raw_f, wl, init, hop in zip(index_id_map, *data):\n",
        "            final_embeddings[_index, :] = np.array(\n",
        "                self.model(raw_f.unsqueeze(0), wl.unsqueeze(0), init.unsqueeze(0), hop.unsqueeze(0))[0]\n",
        "                    .cpu().detach())\n",
        "        return self.get_embeddings_dict(final_embeddings, index_id_map, id2label)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_embeddings_dict(embeddings, index2id_map, id2label):\n",
        "        return {id2label[index]: embeddings[_id] for _id, index in index2id_map.items()}\n",
        "\n",
        "    def save_embeddings(self, test_synsets, embeddings, result_dir):\n",
        "        with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_train_embeddings.txt\"), 'w') as w1:\n",
        "            with open(os.path.join(result_dir, f\"{self.model.__class__.__name__}_model_test_embeddings.txt\"),\n",
        "                      'w') as w2:\n",
        "                for synset_name, embedding in embeddings.items():\n",
        "                    if synset_name in test_synsets:\n",
        "                        text_embedding = \" \".join([str(e) for e in embedding])\n",
        "                        w2.write(f\"{synset_name} {text_embedding}\\n\")\n",
        "                    else:\n",
        "                        text_embedding = \" \".join([str(e) for e in embedding])\n",
        "                        w1.write(f\"{synset_name} {text_embedding}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frvTximja2BI",
        "outputId": "c5262963-a181-4d7e-8553-57196fdf0209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading wordnet_n_is_directed_1_en_synsets_2.0 dataset...\n",
            "Load WL Dictionary\n",
            "Load Hop Distance Dictionary\n",
            "Load Subgraph Batches\n"
          ]
        }
      ],
      "source": [
        "loaded_data = load_data('wordnet_n_is_directed_1_en_synsets_2.0', 5, 'cpu')\n",
        "dataset = (loaded_data['raw_embeddings'], loaded_data['wl_embedding'], loaded_data['hop_embeddings'],\n",
        "           loaded_data['int_embeddings'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uj1GzbATa2BJ"
      },
      "outputs": [],
      "source": [
        "index_id_map = loaded_data['index_id_map']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WetYSlefa2BJ"
      },
      "outputs": [],
      "source": [
        "idx_features_labels = np.genfromtxt(\"{}/node\".format('/home/nikishina/Graph-Bert/data/wordnet_n_is_directed_1_en_synsets_2.0/'), dtype=np.dtype(str))\n",
        "id2label = {int(i): j for i, j in zip(idx_features_labels[:, 0], idx_features_labels[:, -1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahwRGzqZa2BJ",
        "outputId": "3cd43a46-8c3f-405e-b819-28c0706f11df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load pretrained model from ./result/PreTrained_GraphBert/wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model\n"
          ]
        }
      ],
      "source": [
        "saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_reconstruct_model', MethodGraphBertNodeConstruct)\n",
        "saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB-tbeNba2BK",
        "outputId": "4ec2abd1-209f-4865-a146-97b27b1b393f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Load pretrained model from ./result/PreTrained_GraphBert/wordnet_n_is_directed_1_en_synsets_2.0/node_graph_reconstruct_model\n"
          ]
        }
      ],
      "source": [
        "saver = GraphBERTEmbeddingsSaver('wordnet_n_is_directed_1_en_synsets_2.0/node_graph_reconstruct_model', MethodGraphBertGraphRecovery)\n",
        "saver.compute_and_save_embeddings(dataset, new_words, index_id_map, id2label, \"../\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE-7RzJ1a2BK"
      },
      "source": [
        "## View and evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n",
        "!gdown 1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n",
        "!gdown 1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n",
        "!gdown 1h5sSbFeCJbouH96fKIZDF2xugNiKf3La"
      ],
      "metadata": {
        "id": "zvWT_UAaSW7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d90fcd6-2155-46a3-ba03-cf97520e4aeb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IAfd9tRgtVtdosM5vuDdxh-VSBFp3mzI\n",
            "To: /content/MethodGraphBertGraphRecovery_model_test_embeddings.txt\n",
            "100% 258k/258k [00:00<00:00, 96.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LItbxEcchOfU4TrlLBZjQweC8jpQ3b3Q\n",
            "To: /content/MethodGraphBertGraphRecovery_model_train_embeddings_.txt\n",
            "100% 196M/196M [00:02<00:00, 80.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VLLLyu9YyLX3uCojiTm_VLtgK2gKCCfW\n",
            "To: /content/MethodGraphBertNodeConstruct_model_train_embeddings_.txt\n",
            "100% 288M/288M [00:04<00:00, 68.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1h5sSbFeCJbouH96fKIZDF2xugNiKf3La\n",
            "To: /content/MethodGraphBertNodeConstruct_model_test_embeddings.txt\n",
            "100% 362k/362k [00:00<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "uLCgr2ibUZL9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bW9Z8hLca2BL"
      },
      "outputs": [],
      "source": [
        "graphBertNode_train = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_train_embeddings_.txt\")\n",
        "graphBertNode_test = KeyedVectors.load_word2vec_format(\"MethodGraphBertNodeConstruct_model_test_embeddings.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "zdnqyA7ia2BL",
        "outputId": "1602b65d-d6eb-4f16-de51-fa4f0c27b29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hound.n.01', 0.8980603814125061),\n",
              " ('working_dog.n.01', 0.8848254680633545),\n",
              " ('dandy.n.01', 0.8751208782196045),\n",
              " ('old_man.n.01', 0.8639864325523376),\n",
              " ('professional.n.01', 0.8605043888092041),\n",
              " ('gravida.n.02', 0.849956750869751),\n",
              " ('child.n.02', 0.8499069809913635),\n",
              " ('spaniel.n.01', 0.8490362167358398),\n",
              " ('subordinate.n.01', 0.8471304178237915),\n",
              " ('parent.n.01', 0.8452426195144653)]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "graphBertNode_train.similar_by_word(\"dog.n.01\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wn.synset(\"depression.n.01\").hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAEmE4jXwJR6",
        "outputId": "26691d48-25ae-4b4d-bc71-115688c3876c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('psychological_state.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EVHNs_-Za2BM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "addc8167-d8f2-4015-bfcf-90c8fbaa45d6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-916afd8e48bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\".n.\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mgraphbert_node_predicts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morph2par\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraphBertNode_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphBertNode_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morph2par\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \"\"\"\n\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'course.n.04' not present\""
          ]
        }
      ],
      "source": [
        "graphbert_node_predicts = {}\n",
        "\n",
        "for word in fasttext.key_to_index:\n",
        "    if \".n.\" not in word:\n",
        "        graphbert_node_predicts[orph2par[word]] = graphBertNode_train.similar_by_vector(graphBertNode_test[orph2par[word]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VTSr-h0wwo7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of sber_NLP_Graph_Embeddings_and_Neural_Networks-final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}