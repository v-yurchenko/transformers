{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "412bf163-f4a9-4fac-b427-6afa35cb2d26",
   "metadata": {},
   "source": [
    "# cointegrated/rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea40df52-2a68-4f52-96c8-d105c41c73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d7268b-9a2c-450c-861b-2872e5813e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28539de1-4920-4d30-b6fe-dc63f7a48e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, splitter=\"\\t\"):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                word, tag = line.split(splitter)\n",
    "                sentence.append(word)\n",
    "                tags.append(tag.strip())\n",
    "            else:\n",
    "                data.append((sentence, tags))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174d4ad4-7b9e-43e2-80cb-e16dd6ba6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_dataset(\"task1/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5859c07-a991-4ba2-bcff-01c4a97c7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ner_data = pd.DataFrame(training_data, columns=['tokens', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70372af6-2002-4ab1-99ae-74ca79a8aae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[also, ,, i, have, recently, discovered, advil...</td>\n",
       "      <td>[O, O, O, O, O, O, B-Object, O, O, O, B-Predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, have, always, heard, that, motrin, is, bet...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[when, i, was, a, figure, skater, i, injuried,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[in, a, way, ,, halloween, is, even, better, t...</td>\n",
       "      <td>[O, O, O, O, B-Object, O, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, think, halloween, is, actually, safer, tha...</td>\n",
       "      <td>[O, O, B-Object, O, O, B-Predicate, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>[isn, ', t, plastic, safer, than, wood, .]</td>\n",
       "      <td>[O, O, O, B-Object, B-Predicate, O, B-Object, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>[and, plastic, pallets, are, safer, than, wood...</td>\n",
       "      <td>[O, B-Object, O, O, B-Predicate, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>[plastic, laminate, flooring, is, one, of, the...</td>\n",
       "      <td>[B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>[plastic, has, long, been, considered, superio...</td>\n",
       "      <td>[B-Object, O, O, O, O, B-Predicate, O, B-Objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2334 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [also, ,, i, have, recently, discovered, advil...   \n",
       "1     [i, have, always, heard, that, motrin, is, bet...   \n",
       "2     [when, i, was, a, figure, skater, i, injuried,...   \n",
       "3     [in, a, way, ,, halloween, is, even, better, t...   \n",
       "4     [i, think, halloween, is, actually, safer, tha...   \n",
       "...                                                 ...   \n",
       "2329         [isn, ', t, plastic, safer, than, wood, .]   \n",
       "2330  [and, plastic, pallets, are, safer, than, wood...   \n",
       "2331  [plastic, laminate, flooring, is, one, of, the...   \n",
       "2332  [plastic, has, long, been, considered, superio...   \n",
       "2333  [in, the, content, of, search, results, ,, bin...   \n",
       "\n",
       "                                                   tags  \n",
       "0     [O, O, O, O, O, O, B-Object, O, O, O, B-Predic...  \n",
       "1     [O, O, O, O, O, B-Object, O, B-Predicate, O, B...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, B-Object, O, O, B-Predicate, O, B...  \n",
       "4     [O, O, B-Object, O, O, B-Predicate, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "2329   [O, O, O, B-Object, B-Predicate, O, B-Object, O]  \n",
       "2330  [O, B-Object, O, O, B-Predicate, O, B-Object, ...  \n",
       "2331  [B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...  \n",
       "2332  [B-Object, O, O, O, O, B-Predicate, O, B-Objec...  \n",
       "2333  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "\n",
       "[2334 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e75934-a43c-4df3-8940-fe526047007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-Object',\n",
       " 'I-Aspect',\n",
       " 'I-Predicate',\n",
       " 'B-Object',\n",
       " 'B-Predicate',\n",
       " 'B-Aspect']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "for item in ner_data['tags']:\n",
    "    label_list.extend(item)\n",
    "label_list = list(set(label_list))\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8714735-df2c-46c5-984d-57fec98dcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# ner_data = [extract_labels(item) for item in drugs]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c4c6287-6247-4448-8cfb-cfaa563bdfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d00ad31-ae63-4485-bdd4-d9a0e74ae616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 1867\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1219b381-4182-4d27-b179-922e6a671a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "batch_size = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20312d3-87d2-4cde-a736-ea8c00589598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 9944, 16, 881, 550, 835, 15503, 5, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e696dc61-7723-40dc-9851-aa5506994ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b48b29e8-990e-4aa3-9e18-ef87cf2a3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['as', 'scala', 'reduces', 'a', 'lot', 'of', 'code', 'overhead', 'from', 'other', 'languages', 'it', 'might', 'even', 'be', 'easier', 'to', 'learn', 'concepts', 'with', 'scala', 'than', 'with', 'java', '/', 'c', '++.']\n"
     ]
    }
   ],
   "source": [
    "i = i + 1\n",
    "example = ner_dataset['test'][i]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfced5bd-c1fb-4566-9e41-afdf4eba66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "# tokenizer(\"Hello world\")['input_ids']\n",
    "# tokenizer(\" Hello world\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b7e798b-e48f-40da-8167-6699db90c692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 571, 20031, 9480, 533, 68, 5680, 534, 4354, 856, 5894, 610, 979, 6341, 683, 6167, 2513, 747, 23667, 540, 13645, 16031, 594, 20031, 1236, 594, 622, 759, 19, 70, 15, 15, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (0, 5), (0, 6), (6, 7), (0, 1), (0, 3), (0, 2), (0, 4), (0, 4), (4, 8), (0, 4), (0, 5), (0, 9), (0, 2), (0, 5), (0, 4), (0, 2), (0, 6), (0, 2), (0, 5), (0, 8), (0, 4), (0, 5), (0, 4), (0, 4), (0, 2), (2, 4), (0, 1), (0, 1), (0, 1), (1, 2), (2, 3), (0, 0)]}\n",
      "['[CLS]', 'as', 'scala', 'reduce', '##s', 'a', 'lot', 'of', 'code', 'over', '##head', 'from', 'other', 'languages', 'it', 'might', 'even', 'be', 'easier', 'to', 'learn', 'concepts', 'with', 'scala', 'than', 'with', 'ja', '##va', '/', 'c', '+', '+', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True,  return_offsets_mapping=True, )\n",
    "print(tokenized_input)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "PRE_WORD = '##'\n",
    "SOS_TOKEN = \"[CLS]\"\n",
    "EOS_TOKEN = \"[SEP]\"\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f4ba9487-843c-4542-ac06-a8a4f50403ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf73c43c-7f3f-489f-983d-5499e916d839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 537, 13195, 17645, 844, 25194, 2051, 5350, 17, 8, 833, 602, 9243, 602, 1136, 17, 550, 4014, 555, 537, 675, 18312, 1236, 16931, 1622, 2365, 11, 86, 3069, 600, 3873, 1589, 8926, 647, 746, 3938, 2881, 13161, 5441, 18, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 4, 0, 0, 0, 6, -100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, -100, 0, 5, -100, 0, 4, -100, -100, 0, 0, 0, 0, 0, 6, -100, -100, 0, 0, 6, -100, 2, 0, -100]]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_dataset['train'][22:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1c66324-3403-4488-93c8-68e9b616f775",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tokens': ['i',\n",
       "  'also',\n",
       "  'preferred',\n",
       "  'the',\n",
       "  'psp',\n",
       "  'controls',\n",
       "  'to',\n",
       "  'the',\n",
       "  'wii',\n",
       "  'controls',\n",
       "  ',',\n",
       "  'since',\n",
       "  'it',\n",
       "  'was',\n",
       "  'pretty',\n",
       "  'much',\n",
       "  'identical',\n",
       "  'to',\n",
       "  'the',\n",
       "  'original',\n",
       "  'ps2',\n",
       "  'release',\n",
       "  '.'],\n",
       " 'tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-Object',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-Predicate',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-Object',\n",
       "  'O',\n",
       "  'O'],\n",
       " '__index_level_0__': 485,\n",
       " 'input_ids': [2,\n",
       "  76,\n",
       "  772,\n",
       "  18513,\n",
       "  531,\n",
       "  26945,\n",
       "  795,\n",
       "  18426,\n",
       "  540,\n",
       "  531,\n",
       "  6989,\n",
       "  542,\n",
       "  18426,\n",
       "  16,\n",
       "  1682,\n",
       "  683,\n",
       "  560,\n",
       "  29049,\n",
       "  2476,\n",
       "  17932,\n",
       "  540,\n",
       "  531,\n",
       "  1424,\n",
       "  26945,\n",
       "  1012,\n",
       "  2959,\n",
       "  18,\n",
       "  3],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9653f68-ff8e-4d3f-8b22-fb452b87c9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-Object',\n",
       " 'I-Aspect',\n",
       " 'I-Predicate',\n",
       " 'B-Object',\n",
       " 'B-Predicate',\n",
       " 'B-Aspect']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7a98e928-e35d-4561-aeb6-b845badcc902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "97e4b5e6-e499-456f-a8ca-43f238b143d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec4827ec-b1e0-48af-a23b-230aac80a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd110f48-8456-4ae4-973e-61fd46c01065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "783eabcc-6b87-4ace-a1be-cb955edc9bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aspect': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'Object': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'Predicate': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_dataset['train'][4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d66e89e-3b87-4604-b4ea-2c55fee0baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels, inputs = p.predictions, p.label_ids, p.inputs\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "\n",
    "    # send only the first token of each word to the evaluation\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    for prediction, label, tokens in zip(predictions, labels, inputs):\n",
    "        true_predictions.append([])\n",
    "        true_labels.append([])\n",
    "        # print(prediction, label, tokens)\n",
    "        for (p, l, t) in zip(prediction, label, tokens):\n",
    "            # print(l, p, tokenizer.convert_ids_to_tokens(int(t)))\n",
    "            if l != -100 and not tokenizer.convert_ids_to_tokens(int(t)).startswith(PRE_WORD):\n",
    "                # print('append')\n",
    "                true_predictions[-1].append(label_list[p])\n",
    "                true_labels[-1].append(label_list[l])\n",
    "    # print(true_predictions, true_labels)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "00a2fa7b-be98-4b23-b31e-f667d04171c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ac69966-f244-4087-9b10-bb0ca89c0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.2178430557250977,\n",
       " 'eval_precision': 0.047325679081489776,\n",
       " 'eval_recall': 0.14922737306843267,\n",
       " 'eval_f1': 0.07186137982353566,\n",
       " 'eval_accuracy': 0.055314895034988334,\n",
       " 'eval_runtime': 1.103,\n",
       " 'eval_samples_per_second': 423.385,\n",
       " 'eval_steps_per_second': 27.198}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33636818-cfa5-4447-98d5-f77986e2d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð·ÐºÐ°\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d2f2577-23b3-4fca-a27c-82ce213947ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ec2f863-0645-480d-8d19-d0fa1f0373ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a6c8773-6860-4cee-9a54-66ed4bcb2115",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/.imgenv-jupyter-server-dfcbc5cc9913-0/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1867\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2340\n",
      "  Number of trainable parameters = 29098303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 00:34, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.630375</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.060927</td>\n",
       "      <td>0.112976</td>\n",
       "      <td>0.800816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.438474</td>\n",
       "      <td>0.721236</td>\n",
       "      <td>0.556291</td>\n",
       "      <td>0.628116</td>\n",
       "      <td>0.867544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.372815</td>\n",
       "      <td>0.718182</td>\n",
       "      <td>0.697572</td>\n",
       "      <td>0.707727</td>\n",
       "      <td>0.890370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.338667</td>\n",
       "      <td>0.745389</td>\n",
       "      <td>0.731567</td>\n",
       "      <td>0.738414</td>\n",
       "      <td>0.899783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.316740</td>\n",
       "      <td>0.766712</td>\n",
       "      <td>0.744371</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.906365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.298807</td>\n",
       "      <td>0.779144</td>\n",
       "      <td>0.755408</td>\n",
       "      <td>0.767093</td>\n",
       "      <td>0.911696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.289747</td>\n",
       "      <td>0.771316</td>\n",
       "      <td>0.778808</td>\n",
       "      <td>0.775044</td>\n",
       "      <td>0.914279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.285354</td>\n",
       "      <td>0.756281</td>\n",
       "      <td>0.797351</td>\n",
       "      <td>0.776273</td>\n",
       "      <td>0.915362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.273402</td>\n",
       "      <td>0.781826</td>\n",
       "      <td>0.786313</td>\n",
       "      <td>0.784063</td>\n",
       "      <td>0.918860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.268957</td>\n",
       "      <td>0.777298</td>\n",
       "      <td>0.795143</td>\n",
       "      <td>0.786120</td>\n",
       "      <td>0.919943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.264724</td>\n",
       "      <td>0.778929</td>\n",
       "      <td>0.796468</td>\n",
       "      <td>0.787601</td>\n",
       "      <td>0.920693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.263891</td>\n",
       "      <td>0.767745</td>\n",
       "      <td>0.807064</td>\n",
       "      <td>0.786913</td>\n",
       "      <td>0.919860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.258731</td>\n",
       "      <td>0.779254</td>\n",
       "      <td>0.802649</td>\n",
       "      <td>0.790779</td>\n",
       "      <td>0.921693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.257725</td>\n",
       "      <td>0.776405</td>\n",
       "      <td>0.804857</td>\n",
       "      <td>0.790375</td>\n",
       "      <td>0.921526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.255610</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.806623</td>\n",
       "      <td>0.791938</td>\n",
       "      <td>0.922276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.254397</td>\n",
       "      <td>0.775986</td>\n",
       "      <td>0.807506</td>\n",
       "      <td>0.791432</td>\n",
       "      <td>0.922109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.252771</td>\n",
       "      <td>0.777683</td>\n",
       "      <td>0.809272</td>\n",
       "      <td>0.793163</td>\n",
       "      <td>0.922692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.253632</td>\n",
       "      <td>0.770842</td>\n",
       "      <td>0.812362</td>\n",
       "      <td>0.791058</td>\n",
       "      <td>0.922276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.251780</td>\n",
       "      <td>0.779337</td>\n",
       "      <td>0.809272</td>\n",
       "      <td>0.794022</td>\n",
       "      <td>0.923276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.252026</td>\n",
       "      <td>0.775804</td>\n",
       "      <td>0.809713</td>\n",
       "      <td>0.792396</td>\n",
       "      <td>0.922443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2340, training_loss=0.34122982188167733, metrics={'train_runtime': 34.0532, 'train_samples_per_second': 1096.518, 'train_steps_per_second': 68.716, 'total_flos': 32565434271084.0, 'train_loss': 0.34122982188167733, 'epoch': 20.0})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c7e7e22-1c35-4480-8ee4-833a8cf3c78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25202637910842896,\n",
       " 'eval_precision': 0.7758037225042301,\n",
       " 'eval_recall': 0.8097130242825608,\n",
       " 'eval_f1': 0.7923957658241522,\n",
       " 'eval_accuracy': 0.9224425191602799,\n",
       " 'eval_runtime': 0.3318,\n",
       " 'eval_samples_per_second': 1407.466,\n",
       " 'eval_steps_per_second': 90.415,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63a93877-0255-4e14-a45a-fe39346d2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_dataset(\"task1/dev_no_answers.tsv\", splitter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "331a0f57-e42e-4d15-876c-688d79572698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-Aspect\n"
     ]
    }
   ],
   "source": [
    "i_pred = [3, 4, 6]\n",
    "print(label_list[i_pred[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bfe9475c-5192-41f1-ae3c-b8479904d4f8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'of', 'course', ',', 'fox', 'may', 'be', 'even', 'worse', 'than', 'cnn', '.)']\n",
      "['O', 'O', 'O', 'O', 'B-Object', 'O', 'O', 'O', 'B-Predicate', 'O', 'B-Object', 'O']\n",
      "['i', 'have', 'tried', 'windows', '8', 'and', 'it', \"'\", 's', 'lighter', 'than', 'windows', 'xp', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Predicate', 'O', 'B-Object', 'B-Object', 'O']\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence):\n",
    "    inputs = tokenizer(sentence,\n",
    "                        is_split_into_words=True, \n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "    # move to gpu\n",
    "    ids = inputs[\"input_ids\"].to('cuda:0')\n",
    "    mask = inputs[\"attention_mask\"].to('cuda:0')\n",
    "    # forward pass\n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [label_list[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "    prediction = []\n",
    "    # print(inputs[\"offset_mapping\"].squeeze().tolist())\n",
    "    for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "      #only predictions on first word pieces are important\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            prediction.append(token_pred[1])\n",
    "        else:\n",
    "            continue\n",
    "    return prediction\n",
    "\n",
    "sentence = ner_dataset['train'][15]['tokens'] #\"@HuggingFace is a company based in New York, but is also has employees working in Paris\"\n",
    "print(sentence)\n",
    "print(predict_sentence(sentence))\n",
    "\n",
    "print(test_data[2][0])\n",
    "print(predict_sentence(test_data[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7d0dea28-a034-4566-8760-b2836764e498",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283/283 [00:02<00:00, 96.23it/s] \n"
     ]
    }
   ],
   "source": [
    "with open(\"task1/cointegrated-rubert-tiny2.tsv\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            # print(sentence[0])\n",
    "            prediction = predict_sentence(sentence[0])\n",
    "            for w,t in zip(sentence[0], prediction):\n",
    "                # print(w, '\\t', t)\n",
    "                f.write(w+'\\t'+t+'\\n')\n",
    "            f.write('\\n')\n",
    "            # # print(sentence[0])\n",
    "            # tokens = tokenizer(sentence[0], is_split_into_words=True, return_tensors='pt').to(model.device)\n",
    "            # pred = model(**tokens)\n",
    "            # indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "            # token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "            # print((indices), (token_text))\n",
    "            # word_buffer  = ''\n",
    "            # token_buffer = 0\n",
    "            # current_token = 0\n",
    "            # for token, idx in zip(token_text, indices):\n",
    "            #     if idx  in i_pred:\n",
    "            #         print(\"---> \", label_list[idx])\n",
    "            #     # print(token, idx)\n",
    "            #     if token == SOS_TOKEN:\n",
    "            #         continue\n",
    "            #     if word_buffer != '' and not token.startswith(PRE_WORD):\n",
    "            #         print(word_buffer + \"\\t\" + label_list[token_buffer])\n",
    "            #         f.write(word_buffer + \"\\t\" + label_list[token_buffer] + \"\\n\")\n",
    "            #         current_token = token_buffer\n",
    "            #         word_buffer = ''\n",
    "            #         token_buffer = 0\n",
    "            #     if token == EOS_TOKEN:\n",
    "            #         continue\n",
    "            #         # print(wb[1:] + \"\\t\" + label_list[tb])\n",
    "            #         # f.write(word_buffer + \"\\t\" + label_list[token_buffer] + \"\\n\")\n",
    "            #     if token.startswith(PRE_WORD):\n",
    "            #         word_buffer += token[2:]\n",
    "            #     else:\n",
    "            #         word_buffer += token\n",
    "            #     if token_buffer == 0:\n",
    "            #         token_buffer = idx\n",
    "            # print(\"\\n\")    \n",
    "            # f.write(\"\\n\")\n",
    "                # print(f'{t:15s} {label_list[idx]:10s}')\n",
    "            # inputs = tokenizer(prepare_sequence(sentence[0], word_to_ix)\n",
    "            # tag_scores = model(inputs)\n",
    "            # tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "            # for i, y in zip(sentence[0], tags):\n",
    "            #     w.write(f\"{i}\\t{y}\\n\")\n",
    "            # w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "c7b78e5c-514e-44b5-8864-7f209ea4b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "previos_bio = 'O'\n",
    "\n",
    "with open(\"task1/cointegrated-rubert-tiny2.tsv\", \"r\") as f:\n",
    "    with open(\"task1/cointegrated-rubert-tiny2-fix.tsv\", \"w\") as fw:\n",
    "        for line in f:\n",
    "            line = line.strip('\\r\\n').split('\\t')\n",
    "            if len(line) > 1:\n",
    "                word, bio = line\n",
    "                current_bio = bio\n",
    "                if current_bio[1:] == previos_bio[1:] and (previos_bio[:1] == 'B' or previos_bio[:1] == 'I'):\n",
    "                    current_bio = 'I'+current_bio[1:]\n",
    "                fw.write(word + '\\t'+current_bio+'\\n')\n",
    "                previos_bio = current_bio\n",
    "            else:\n",
    "                fw.write(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "faaadeb2-228e-4db7-b6df-3d523422dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: zip: command not found\n"
     ]
    }
   ],
   "source": [
    "!zip out.zip task1/out_test_roberta_large.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebb747-bc20-4531-86ef-98a78dd47970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c9f12e-df8d-4c2b-9b57-116479ae695b",
   "metadata": {},
   "source": [
    "# sberbank-ai/ruRoberta-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a3bf12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1d81804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, splitter=\"\\t\"):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                word, tag = line.split(splitter)\n",
    "                sentence.append(word)\n",
    "                tags.append(tag.strip())\n",
    "            else:\n",
    "                data.append((sentence, tags))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cdb89fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_dataset(\"task1/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "00389d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ner_data = pd.DataFrame(training_data, columns=['tokens', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "83d54190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[also, ,, i, have, recently, discovered, advil...</td>\n",
       "      <td>[O, O, O, O, O, O, B-Object, O, O, O, B-Predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[i, have, always, heard, that, motrin, is, bet...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[when, i, was, a, figure, skater, i, injuried,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[in, a, way, ,, halloween, is, even, better, t...</td>\n",
       "      <td>[O, O, O, O, B-Object, O, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, think, halloween, is, actually, safer, tha...</td>\n",
       "      <td>[O, O, B-Object, O, O, B-Predicate, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>[isn, ', t, plastic, safer, than, wood, .]</td>\n",
       "      <td>[O, O, O, B-Object, B-Predicate, O, B-Object, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>[and, plastic, pallets, are, safer, than, wood...</td>\n",
       "      <td>[O, B-Object, O, O, B-Predicate, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>[plastic, laminate, flooring, is, one, of, the...</td>\n",
       "      <td>[B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>[plastic, has, long, been, considered, superio...</td>\n",
       "      <td>[B-Object, O, O, O, O, B-Predicate, O, B-Objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2334 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [also, ,, i, have, recently, discovered, advil...   \n",
       "1     [i, have, always, heard, that, motrin, is, bet...   \n",
       "2     [when, i, was, a, figure, skater, i, injuried,...   \n",
       "3     [in, a, way, ,, halloween, is, even, better, t...   \n",
       "4     [i, think, halloween, is, actually, safer, tha...   \n",
       "...                                                 ...   \n",
       "2329         [isn, ', t, plastic, safer, than, wood, .]   \n",
       "2330  [and, plastic, pallets, are, safer, than, wood...   \n",
       "2331  [plastic, laminate, flooring, is, one, of, the...   \n",
       "2332  [plastic, has, long, been, considered, superio...   \n",
       "2333  [in, the, content, of, search, results, ,, bin...   \n",
       "\n",
       "                                                   tags  \n",
       "0     [O, O, O, O, O, O, B-Object, O, O, O, B-Predic...  \n",
       "1     [O, O, O, O, O, B-Object, O, B-Predicate, O, B...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, B-Object, O, O, B-Predicate, O, B...  \n",
       "4     [O, O, B-Object, O, O, B-Predicate, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "2329   [O, O, O, B-Object, B-Predicate, O, B-Object, O]  \n",
       "2330  [O, B-Object, O, O, B-Predicate, O, B-Object, ...  \n",
       "2331  [B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...  \n",
       "2332  [B-Object, O, O, O, O, B-Predicate, O, B-Objec...  \n",
       "2333  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "\n",
       "[2334 rows x 2 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e12c4c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-Object',\n",
       " 'I-Aspect',\n",
       " 'I-Predicate',\n",
       " 'B-Object',\n",
       " 'B-Predicate',\n",
       " 'B-Aspect']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "for item in ner_data['tags']:\n",
    "    label_list.extend(item)\n",
    "label_list = list(set(label_list))\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "21063048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# ner_data = [extract_labels(item) for item in drugs]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "4009ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "68531ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 1867\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e196d105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/vocab.json\n",
      "loading file merges.txt from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/merges.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "model_checkpoint = \"sberbank-ai/ruRoberta-large\"\n",
    "# model_checkpoint = \"liaad/srl-en_xlmr-large\"\n",
    "batch_size = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "21a07567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 3449, 9691, 83, 16, 34952, 16710, 38237, 2466, 4271, 15922, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4a836517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'sony', 'announced', 'at', 'e3', 'how', 'much', 'better', 'it', 'was', 'than', 'microsoft', ',', 'it', 'reaffirmed', 'that', 'the', 'ps4', 'would', 'be', 'region', 'free', '.']\n"
     ]
    }
   ],
   "source": [
    "example = ner_dataset['train'][6]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c9e03a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizerFast\n",
    "# tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\", add_prefix_space=True)\n",
    "# tokenizer(\"Hello world\")['input_ids']\n",
    "# tokenizer(\" Hello world\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "150e9f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Ä wh', 'en', 'Ä s', 'ony', 'Ä an', 'n', 'ou', 'n', 'c', 'ed', 'Ä at', 'Ä e', '3', 'Ä h', 'ow', 'Ä m', 'uch', 'Ä b', 'et', 'ter', 'Ä it', 'Ä was', 'Ä th', 'an', 'Ä m', 'ic', 'rosoft', 'Ä ,', 'Ä it', 'Ä re', 'a', 'ff', 'ir', 'm', 'ed', 'Ä that', 'Ä the', 'Ä p', 's', '4', 'Ä would', 'Ä be', 'Ä re', 'g', 'ion', 'Ä f', 'ree', 'Ä .', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "PRE_WORD = 'Ä ' # '##'\n",
    "# PRE_WORD = '_'\n",
    "SOS_TOKEN = \"<s>\"\n",
    "EOS_TOKEN = \"</s>\"\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c8eb03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ceb3cb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 4810, 6340, 14519, 4118, 3267, 5694, 49019, 14753, 87, 20498, 3893, 15808, 26514, 21145, 2595, 3633, 406, 7061, 4996, 33684, 33958, 1188, 33684, 10623, 15446, 406, 16710, 2466, 4458, 20433, 1188, 4810, 3157, 1892, 7420, 1188, 7064, 1429, 3525, 2595, 31963, 8442, 2466, 4179, 74, 18844, 7822, 42491, 5747, 73, 4394, 2595, 11535, 3633, 28347, 2466, 23744, 91, 15964, 225, 14076, 4140, 24520, 22300, 75, 19405, 87, 3026, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 4, -100, 0, -100, -100, -100, 0, -100, 0, -100, -100, -100, 6, -100, -100, 0, 0, 0, 0, 0, -100, 0, 0, -100, 0, 0, 5, -100, -100, -100, 0, 5, -100, -100, -100, 0, -100, 4, -100, -100, 0, 0, 0, -100, -100, 0, 0, -100, -100, 6, -100, -100, -100, 0, 0, -100, -100, -100, 6, -100, -100, -100, 2, -100, -100, -100, 0, -100]]}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(ner_dataset['train'][22:23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "19191228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "7aa6e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'I-Object',\n",
       " 'I-Aspect',\n",
       " 'I-Predicate',\n",
       " 'B-Object',\n",
       " 'B-Predicate',\n",
       " 'B-Aspect']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "5c71b40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sberbank-ai/ruRoberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/hub/models--sberbank-ai--ruRoberta-large/snapshots/29b46edec511391c384dfd0bbd3892cb72495c5f/pytorch_model.bin\n",
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3f1fef35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "733ad0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "503fb0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "93db0199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aspect': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'Object': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'Predicate': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_dataset['train'][4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7350ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels, inputs = p.predictions, p.label_ids, p.inputs\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "\n",
    "    # send only the first token of each word to the evaluation\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    for prediction, label, tokens in zip(predictions, labels, inputs):\n",
    "        true_predictions.append([])\n",
    "        true_labels.append([])\n",
    "        # print(prediction, label, tokens)\n",
    "        for (p, l, t) in zip(prediction, label, tokens):\n",
    "            # print(l, p, tokenizer.convert_ids_to_tokens(int(t)))\n",
    "            if l != -100 and tokenizer.convert_ids_to_tokens(int(t)).startswith(PRE_WORD):\n",
    "                # print('append')\n",
    "                true_predictions[-1].append(label_list[p])\n",
    "                true_labels[-1].append(label_list[l])\n",
    "    # print(true_predictions, true_labels)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8c3cd667",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "21ecec3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6836447715759277,\n",
       " 'eval_precision': 0.05716265373289451,\n",
       " 'eval_recall': 0.1456953642384106,\n",
       " 'eval_f1': 0.08210997760636975,\n",
       " 'eval_accuracy': 0.33063978673775407,\n",
       " 'eval_runtime': 2.6374,\n",
       " 'eval_samples_per_second': 177.069,\n",
       " 'eval_steps_per_second': 11.375}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "59840a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ñ€Ð°Ð·Ð¼Ð¾Ñ€Ð¾Ð·ÐºÐ°\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "dbcd1c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "121a5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0566b045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/.imgenv-jupyter-server-dfcbc5cc9913-0/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1867\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2340\n",
      "  Number of trainable parameters = 354317319\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 11:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.196540</td>\n",
       "      <td>0.816631</td>\n",
       "      <td>0.845475</td>\n",
       "      <td>0.830803</td>\n",
       "      <td>0.934439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.171772</td>\n",
       "      <td>0.835448</td>\n",
       "      <td>0.871965</td>\n",
       "      <td>0.853316</td>\n",
       "      <td>0.939687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.171554</td>\n",
       "      <td>0.845990</td>\n",
       "      <td>0.875497</td>\n",
       "      <td>0.860490</td>\n",
       "      <td>0.943685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193692</td>\n",
       "      <td>0.813897</td>\n",
       "      <td>0.884327</td>\n",
       "      <td>0.847651</td>\n",
       "      <td>0.936688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.192969</td>\n",
       "      <td>0.845601</td>\n",
       "      <td>0.882561</td>\n",
       "      <td>0.863685</td>\n",
       "      <td>0.945685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.209315</td>\n",
       "      <td>0.838994</td>\n",
       "      <td>0.883444</td>\n",
       "      <td>0.860645</td>\n",
       "      <td>0.943519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.233944</td>\n",
       "      <td>0.864133</td>\n",
       "      <td>0.873289</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.947018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.251467</td>\n",
       "      <td>0.826069</td>\n",
       "      <td>0.886976</td>\n",
       "      <td>0.855440</td>\n",
       "      <td>0.942019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.268532</td>\n",
       "      <td>0.832849</td>\n",
       "      <td>0.886534</td>\n",
       "      <td>0.858854</td>\n",
       "      <td>0.943269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.296044</td>\n",
       "      <td>0.837034</td>\n",
       "      <td>0.882119</td>\n",
       "      <td>0.858985</td>\n",
       "      <td>0.944269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.296565</td>\n",
       "      <td>0.817851</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.854250</td>\n",
       "      <td>0.944102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.311928</td>\n",
       "      <td>0.816514</td>\n",
       "      <td>0.903753</td>\n",
       "      <td>0.857921</td>\n",
       "      <td>0.943019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.316712</td>\n",
       "      <td>0.852515</td>\n",
       "      <td>0.883002</td>\n",
       "      <td>0.867491</td>\n",
       "      <td>0.945518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.332679</td>\n",
       "      <td>0.837984</td>\n",
       "      <td>0.888300</td>\n",
       "      <td>0.862409</td>\n",
       "      <td>0.944685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.347697</td>\n",
       "      <td>0.849787</td>\n",
       "      <td>0.881678</td>\n",
       "      <td>0.865439</td>\n",
       "      <td>0.946185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.359116</td>\n",
       "      <td>0.840634</td>\n",
       "      <td>0.889625</td>\n",
       "      <td>0.864436</td>\n",
       "      <td>0.946351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.362075</td>\n",
       "      <td>0.839899</td>\n",
       "      <td>0.884768</td>\n",
       "      <td>0.861750</td>\n",
       "      <td>0.945435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.365687</td>\n",
       "      <td>0.847515</td>\n",
       "      <td>0.888300</td>\n",
       "      <td>0.867428</td>\n",
       "      <td>0.947184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.377640</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>0.886534</td>\n",
       "      <td>0.866264</td>\n",
       "      <td>0.947351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.376921</td>\n",
       "      <td>0.847752</td>\n",
       "      <td>0.882561</td>\n",
       "      <td>0.864806</td>\n",
       "      <td>0.946768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2340, training_loss=0.05253812930522821, metrics={'train_runtime': 673.7028, 'train_samples_per_second': 55.425, 'train_steps_per_second': 3.473, 'total_flos': 7060346142497208.0, 'train_loss': 0.05253812930522821, 'epoch': 20.0})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9c5cae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__, tokens, tags. If __index_level_0__, tokens, tags are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 467\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3769207000732422,\n",
       " 'eval_precision': 0.8477523324851569,\n",
       " 'eval_recall': 0.882560706401766,\n",
       " 'eval_f1': 0.864806402768765,\n",
       " 'eval_accuracy': 0.946767744085305,\n",
       " 'eval_runtime': 2.6211,\n",
       " 'eval_samples_per_second': 178.168,\n",
       " 'eval_steps_per_second': 11.445,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6bc1c1b4-5c69-4a3f-a606-3b77a51f7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_dataset(\"task1/dev_no_answers.tsv\", splitter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0eb2e73a-6dff-49d1-a030-2943015078b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283/283 [00:03<00:00, 90.55it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"task1/out_test_roberta_large.tsv\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            # print(sentence[0])\n",
    "            prediction = predict_sentence(sentence[0])\n",
    "            for w,t in zip(sentence[0], prediction):\n",
    "                # print(w, '\\t', t)\n",
    "                f.write(w+'\\t'+t+'\\n')\n",
    "            f.write('\\n')\n",
    "#     with torch.no_grad():\n",
    "#         for sentence in tqdm(test_data):\n",
    "#             # print(sentence[0])\n",
    "#             tokens = tokenizer(sentence[0], is_split_into_words=True, return_tensors='pt').to(model.device)\n",
    "#             pred = model(**tokens)\n",
    "#             indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "#             token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "#             # print((indices), (token_text))\n",
    "#             wb = ''\n",
    "#             tb = 0\n",
    "#             for t, idx in zip(token_text, indices):\n",
    "#                 # print(t, idx)\n",
    "#                 if t == SOS_TOKEN:\n",
    "#                     continue\n",
    "#                 if wb != '' and t.startswith(PRE_WORD):\n",
    "#                     # print(wb[1:] + \"\\t\" + label_list[tb])\n",
    "#                     f.write(wb[1:] + \"\\t\" + label_list[tb] + \"\\n\")\n",
    "#                     wb = ''\n",
    "#                     tb = 0\n",
    "#                 if t == EOS_TOKEN:\n",
    "#                     # print(wb[1:] + \"\\t\" + label_list[tb])\n",
    "#                     f.write(wb[1:] + \"\\t\" + label_list[tb] + \"\\n\")\n",
    "#                 wb += t\n",
    "#                 if tb == 0:\n",
    "#                     tb = idx\n",
    "                \n",
    "#             f.write(\"\\n\")\n",
    "#                 # print(f'{t:15s} {label_list[idx]:10s}')\n",
    "#             # inputs = tokenizer(prepare_sequence(sentence[0], word_to_ix)\n",
    "#             # tag_scores = model(inputs)\n",
    "#             # tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "#             # for i, y in zip(sentence[0], tags):\n",
    "#             #     w.write(f\"{i}\\t{y}\\n\")\n",
    "#             # w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "efea66da-6a0f-41ae-99e7-dde9362fccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct I->B\n",
      "greater I-Predicate O\n",
      "greater B-Predicate O\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct I->B\n",
      "drops I-Aspect O\n",
      "drops B-Aspect O\n",
      "correct B->I\n",
      "correct I->B\n",
      "out I-Aspect O\n",
      "out B-Aspect O\n",
      "correct I->B\n",
      "of I-Aspect O\n",
      "of B-Aspect O\n",
      "correct I->B\n",
      "carolina I-Object O\n",
      "carolina B-Object O\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct I->B\n",
      "prices I-Aspect O\n",
      "prices B-Aspect O\n",
      "correct B->I\n",
      "correct I->B\n",
      "email I-Aspect O\n",
      "email B-Aspect O\n",
      "correct B->I\n",
      "correct I->B\n",
      "ipad I-Object O\n",
      "ipad B-Object O\n",
      "correct I->B\n",
      "fast I-Predicate O\n",
      "fast B-Predicate O\n",
      "correct B->I\n",
      "correct I->B\n",
      "faster I-Predicate O\n",
      "faster B-Predicate O\n",
      "correct B->I\n",
      "correct I->B\n",
      "erection I-Aspect O\n",
      "erection B-Aspect O\n",
      "correct I->B\n",
      "- I-Object O\n",
      "- B-Object O\n",
      "correct B->I\n",
      "correct I->B\n",
      "drive I-Predicate O\n",
      "drive B-Predicate O\n",
      "correct B->I\n",
      "correct I->B\n",
      "starliner I-Object O\n",
      "starliner B-Object O\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct I->B\n",
      "money I-Aspect O\n",
      "money B-Aspect O\n",
      "correct I->B\n",
      "shoe I-Aspect O\n",
      "shoe B-Aspect O\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "tags = []\n",
    "prev_tags = []\n",
    "with open(\"task1/out_test_roberta_large.tsv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip(\"\\r\\n\").split(\"\\t\")\n",
    "        if len(line) > 1:\n",
    "            words.append(line[0])\n",
    "            tags.append(line[1])\n",
    "        else:\n",
    "            words.append(line[0])\n",
    "            tags.append('')\n",
    "\n",
    "with open(\"task1/out_test_roberta_large_fix.tsv\", \"w\") as f:\n",
    "    prev_tags = tags.copy()\n",
    "    prev_tags.insert(0, 'O')\n",
    "    # print(prev_tags)\n",
    "    for w, t, p in zip(words, tags, prev_tags):\n",
    "        # print(w,t,p)\n",
    "        if p.startswith('O') and t.startswith('I'):\n",
    "            print(\"correct I->B\")\n",
    "            print(w, t, p)\n",
    "            t = 'B' + t[1:]\n",
    "            print(w, t, p)\n",
    "        if t.startswith('B') and t == p:\n",
    "            print(\"correct B->I\")\n",
    "            t = 'I' + t[1:]\n",
    "        if w == '':\n",
    "            f.write('\\n')\n",
    "        else:\n",
    "            f.write(w + '\\t'+ t + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e6b4f37d-7738-473c-80da-4a49b8b8b0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/bin/bash: zip: command not found\n"
     ]
    }
   ],
   "source": [
    "!zip out.zip task1/out_test_roberta_large.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7e076-86f3-4253-aca1-0e36a8abe063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
