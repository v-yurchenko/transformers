{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6477a6-f9e3-4363-82b7-7fed6ca56188",
   "metadata": {},
   "source": [
    "# 1. Information about the submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbdf8b-e3c3-4041-b2a8-bbb26fd8c755",
   "metadata": {},
   "source": [
    "## 1.1 Name and number of the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6570d-32b6-496d-b995-612516d30862",
   "metadata": {},
   "source": [
    "Task 1. Comparison Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15a328-e3c5-4963-bb20-102276b8a307",
   "metadata": {},
   "source": [
    "## 1.2 Student name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1276bfb-456e-4566-9f34-bc641c1d2f99",
   "metadata": {},
   "source": [
    "Yurchenko Vladislav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce182eb6-da2d-4e24-924e-040e6355ea7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.3 Codalab user ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c04a41-8207-4bee-b717-134598cfb153",
   "metadata": {},
   "source": [
    "v-yurchenko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5af556-ff16-4cf5-a5f8-edbf2a049885",
   "metadata": {},
   "source": [
    "# 2. Technical Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e9c9b-9233-4077-a154-47b46775e850",
   "metadata": {},
   "source": [
    "## 2.1 Methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de82a59f-11ef-47f2-9564-0ded07c89f12",
   "metadata": {},
   "source": [
    "Задача решалась с помощью дообучения моделей (fine-tuning) семейства BERT.\n",
    "Подбирались более и менее мощные модели. \n",
    "Проводился подбор гиперпараметров: \n",
    " - размер батча\n",
    " - learning-rate\n",
    " - количество эпох\n",
    " - количество слоев для разморозки\n",
    " - параметры Dropout\n",
    "Для улучшения качества модели использовался подход аугментации датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360f65f-52cd-47eb-907d-b3f0fb1f3e2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.2 Discussion of results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4ec29d-a560-42db-ae03-dd57ae1923c2",
   "metadata": {},
   "source": [
    "Мои эксперименты:\n",
    "\n",
    "| Checkpoint | Additional info | F1-average on dev | F1 on test |\n",
    "| :- | :- | -: | -: |\n",
    "| LSTM | lilaspourpre-baseline | - | 0.535994 |\n",
    "| dslim/bert-base-NER | - | 0.724906 | - |\n",
    "| distilbert-base-uncased | - | 0.741844 | 0.826632 |\n",
    "| distilbert-base-uncased | Augment 1 word |  0.718097 | - |\n",
    "| distilbert-base-uncased | Augment 3 word |  0.728500 | - |\n",
    "| cointegrated/rubert-tiny2 | - | 0.612306 | |\n",
    "| nsi319/distilbert-base-uncased-finetuned-app | - | 0.717692 | - |\n",
    "| bert-base-uncased | - | 0.744809 | - |\n",
    "| bert-base-uncased | lr=2e-5, epoch=7 |  0.755394 | 0.840702 |\n",
    "| bert-base-uncased | dropout=0.5, 0.2 |  0.760007 | 0.84868 |\n",
    "| **bert-base-uncased** | **dropout=0.5, 0.5, use dev set in train** |  0.760007 | **0.8515** |\n",
    "| xlm-roberta-base | - | 0.654820 | - |\n",
    "| liaad-srl-en_xlmr-large_report| - | 0.363216 | - |\n",
    "| sberbank-ai-ruRoberta-large| - | 0.745017 | - |\n",
    "| **microsoft/deberta-v3-base** | lr=5e-5, epoch=3, dropout = 0.2 | **0.806269** | **0.870761** |\n",
    "| **microsoft/deberta-v3-base** | lr=5e-5, epoch=3, dropout = 0.2 + dev in train | - | **0.871298** |\n",
    "\n",
    "\n",
    "В таблице приведены неиболее интересные результаты, промежуточные варианты опущены.\n",
    "\n",
    "Общие замечания:\n",
    " - Хорошо обучается BERT-base-uncased (не мультиязычный),\n",
    " - Неожиданно в последний момент выстрелила microsoft/deberta-v3-base, которая на dev в обычном пайплайне дала +5% на f1-average, что достаточно много!\n",
    " - Модель достаточно быстро дообучается под датасет, оптимальными параметрами выглядят:\n",
    "   - config.hidden_dropout_prob = 0.2\n",
    "   - attention_probs_dropout_prob = 0.2\n",
    "   - epoch = 3\n",
    "   - learning_rate = 5e-5\n",
    "   - batch_size = 16\n",
    " - В принципе можно использовать более медленный lr=2e-5, но epochs = 7\n",
    " - При увеличении размера батча до 32 качество падает.\n",
    " - Аугментация датасета не дает значимого прироста, приводит к быстрому переобучению модели.\n",
    " - Усложнение модели не дает роста в качестве модели. Берт из коробки сразу дал хорошее качество, его тюнинг улучшил показатели. Кажется, что более сложные модели находят больше Object и Aspect, но они в разметке помечены как O. \n",
    " - В целом возникло множество вопросов к разметке.\n",
    "\n",
    "**Датасет**\n",
    "- Датасет достаточно небольшой и несбалансирован по классам. Хотелось доразметить самому. Есть очевидные ошибки в разметке dev.tsv (windows то размечается B-Object, то нет).\n",
    "- На dev метрики просаживаются. На test хорошо коррелируют с train, если модель не переобучается!\n",
    "- Распределение токенов - коррелирует с метриками качества на лидерборде по типам объектов - чем меньше объектов в выборке, тем ниже метрика\n",
    "\n",
    "| Type | Count | Percent |\n",
    "| :- | -: | -: |\n",
    "| O | 48512 | 0.7943% |\n",
    "| B-Object | 6174 | 0.1011% |\n",
    "| B-Predicate | 3109 | 0.05091% |\n",
    "| B-Aspect | 2069 | 0.03388% |\n",
    "| I-Aspect | 591 | 0.009677% |\n",
    "| I-Predicate | 427 | 0.006992%|\n",
    "| I-Object | 192 | 0.003144%|\n",
    "\n",
    "\n",
    "Какие идеи еще можно было проработать более детально:\n",
    "\n",
    "**Идея 1**. Аугментация:\n",
    " - берем слова помеченные тегами в разметке\n",
    " - маскируем случайным образом от 1 до 3-х слов токеном [MASK]\n",
    " - с помощью модели bert-base-uncased восстанавливаем слово\n",
    " - записываем в датасет\n",
    "Переобучение на такой аугментации возникает из-за того, что я адекватно не делил train / test, чтобы аугментированные версии одного и того же предложения не попадали в разные наборы.\n",
    "\n",
    "**Идея 2**. Предсказания модели в ряде случаев начинались с некорректного тега - I-Object вместо B-Object. Для таких кейсов подправлена разметка функцией fix_bio_in_file\n",
    "\n",
    "**Идея 3**. Использовать доп. информацию о pos_tags из nltk\n",
    " - B-Object относится к тегу NN\n",
    " - B-Predicate - к JJ\n",
    " - B-Aspect - к VB\n",
    "\n",
    "**Идея 4**. Использовать KeyBERT для извлечения из предложения устойчивых словосочетаний (важных для предложения) и считать их кандидатами на разметку. Вместе с идеей 3 это может дать увеличение метрики.\n",
    "\n",
    "**Предложение для разбора на лекции:** Не разобрался как можно переопределить работу с HuggingFace для последовательностей чтобы использовать несколько разнородных последовательностей, а не только текст. Было бы классно услышать как это делать просто через интерфейс HF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defce71-9352-4e3c-8aaf-4414e4617ab7",
   "metadata": {},
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee67d7-5571-4463-9474-01234a1757c1",
   "metadata": {},
   "source": [
    "## 3.2 Requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2f9f16-a13c-4005-a486-60650393619d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlpaug in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (1.1.11)\n",
      "Requirement already satisfied: transformers in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (4.28.1)\n",
      "Requirement already satisfied: datasets in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: seqeval in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from nlpaug) (1.21.6)\n",
      "Requirement already satisfied: gdown>=4.0.0 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from nlpaug) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.22.0 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from nlpaug) (2.28.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /home/user/conda/lib/python3.7/site-packages (from nlpaug) (1.3.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/conda/lib/python3.7/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/user/conda/lib/python3.7/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/user/conda/lib/python3.7/site-packages (from seqeval) (1.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/user/conda/lib/python3.7/site-packages (from gdown>=4.0.0->nlpaug) (4.10.0)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from gdown>=4.0.0->nlpaug) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/conda/lib/python3.7/site-packages (from pandas>=1.2.0->nlpaug) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/user/conda/lib/python3.7/site-packages (from pandas>=1.2.0->nlpaug) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (2021.10.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.22.0->nlpaug) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlpaug transformers datasets seqeval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380a6e6c-27db-48e3-a8dd-665eb9134d9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘task1/’: File exists\n",
      "mkdir: cannot create directory ‘task1/evaluation/’: File exists\n",
      "--2023-04-25 15:18:55--  https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/train.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 599430 (585K) [text/plain]\n",
      "Saving to: ‘task1/train.tsv’\n",
      "\n",
      "task1/train.tsv     100%[===================>] 585.38K  3.56MB/s    in 0.2s    \n",
      "\n",
      "2023-04-25 15:18:56 (3.56 MB/s) - ‘task1/train.tsv’ saved [599430/599430]\n",
      "\n",
      "--2023-04-25 15:18:56--  https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/dev_no_answers.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 43071 (42K) [text/plain]\n",
      "Saving to: ‘task1/dev_no_answers.tsv’\n",
      "\n",
      "task1/dev_no_answer 100%[===================>]  42.06K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-04-25 15:18:56 (1008 KB/s) - ‘task1/dev_no_answers.tsv’ saved [43071/43071]\n",
      "\n",
      "--2023-04-25 15:18:57--  https://raw.githubusercontent.com/v-yurchenko/transformers/main/task1/dev.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 81124 (79K) [text/plain]\n",
      "Saving to: ‘task1/dev.tsv’\n",
      "\n",
      "task1/dev.tsv       100%[===================>]  79.22K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-04-25 15:18:57 (1.49 MB/s) - ‘task1/dev.tsv’ saved [81124/81124]\n",
      "\n",
      "--2023-04-25 15:18:57--  https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/test_no_answers.tsv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 58177 (57K) [text/plain]\n",
      "Saving to: ‘task1/test_no_answers.tsv’\n",
      "\n",
      "task1/test_no_answe 100%[===================>]  56.81K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2023-04-25 15:18:58 (1.26 MB/s) - ‘task1/test_no_answers.tsv’ saved [58177/58177]\n",
      "\n",
      "--2023-04-25 15:18:58--  https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/evaluation/evaluate_f1_partial.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1861 (1.8K) [text/plain]\n",
      "Saving to: ‘evaluate_f1_partial.py’\n",
      "\n",
      "evaluate_f1_partial 100%[===================>]   1.82K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-04-25 15:18:58 (22.4 MB/s) - ‘evaluate_f1_partial.py’ saved [1861/1861]\n",
      "\n",
      "--2023-04-25 15:18:58--  https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/evaluation/f1_score_partial.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 29143 (28K) [text/plain]\n",
      "Saving to: ‘f1_score_partial.py’\n",
      "\n",
      "f1_score_partial.py 100%[===================>]  28.46K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2023-04-25 15:18:59 (3.09 MB/s) - ‘f1_score_partial.py’ saved [29143/29143]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir task1/\n",
    "!mkdir task1/evaluation/\n",
    "!wget -O task1/train.tsv https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/train.tsv\n",
    "!wget -O task1/dev_no_answers.tsv https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/dev_no_answers.tsv\n",
    "!wget -O task1/dev.tsv https://raw.githubusercontent.com/v-yurchenko/transformers/main/task1/dev.tsv\n",
    "!wget -O task1/test_no_answers.tsv https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/test_no_answers.tsv\n",
    "!wget -O evaluate_f1_partial.py https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/evaluation/evaluate_f1_partial.py\n",
    "!wget -O f1_score_partial.py https://raw.githubusercontent.com/s-nlp/semantic-role-labelling/main/evaluation/f1_score_partial.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c757d2-8cb3-4e25-9b70-c8d33c04948f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 25 15:49:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-PCIE-40GB      On   | 00000000:A2:00.0 Off |                    0 |\n",
      "| N/A   49C    P0    41W / 250W |      0MiB / 40537MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83114088-0453-4f1c-935c-f338d986fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bf62c1c4-e6da-4431-9126-a5a5ccbcf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "12287cd4-e3ef-4aa0-a23f-0c55c3324bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, splitter=\"\\t\"):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                word, tag = line.split(splitter)\n",
    "                sentence.append(word)\n",
    "                tags.append(tag.strip())\n",
    "            else:\n",
    "                data.append((sentence, tags))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0482007d-aa44-4420-9796-c795d9dfdc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_dataset(\"task1/train.tsv\")\n",
    "validation_data = read_dataset(\"task1/dev.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d26d7de7-0bc0-4f29-a039-2f1872b1ebed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2334, 2), (283, 2))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ner_data = pd.DataFrame(training_data, columns=['tokens', 'tags'])\n",
    "val_data = pd.DataFrame(validation_data, columns=['tokens', 'tags'])\n",
    "ner_data.shape, val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22103f1c-3534-4988-8275-b37f23721c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[also, ,, i, have, recently, discovered, advil...</td>\n",
       "      <td>[O, O, O, O, O, O, B-Object, O, O, O, B-Predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, have, always, heard, that, motrin, is, bet...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[when, i, was, a, figure, skater, i, injuried,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[in, a, way, ,, halloween, is, even, better, t...</td>\n",
       "      <td>[O, O, O, O, B-Object, O, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[i, think, halloween, is, actually, safer, tha...</td>\n",
       "      <td>[O, O, B-Object, O, O, B-Predicate, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>2329</td>\n",
       "      <td>[isn, ', t, plastic, safer, than, wood, .]</td>\n",
       "      <td>[O, O, O, B-Object, B-Predicate, O, B-Object, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>2330</td>\n",
       "      <td>[and, plastic, pallets, are, safer, than, wood...</td>\n",
       "      <td>[O, B-Object, O, O, B-Predicate, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>2331</td>\n",
       "      <td>[plastic, laminate, flooring, is, one, of, the...</td>\n",
       "      <td>[B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332</th>\n",
       "      <td>2332</td>\n",
       "      <td>[plastic, has, long, been, considered, superio...</td>\n",
       "      <td>[B-Object, O, O, O, O, B-Predicate, O, B-Objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2333</th>\n",
       "      <td>2333</td>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2334 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             tokens  \\\n",
       "0         0  [also, ,, i, have, recently, discovered, advil...   \n",
       "1         1  [i, have, always, heard, that, motrin, is, bet...   \n",
       "2         2  [when, i, was, a, figure, skater, i, injuried,...   \n",
       "3         3  [in, a, way, ,, halloween, is, even, better, t...   \n",
       "4         4  [i, think, halloween, is, actually, safer, tha...   \n",
       "...     ...                                                ...   \n",
       "2329   2329         [isn, ', t, plastic, safer, than, wood, .]   \n",
       "2330   2330  [and, plastic, pallets, are, safer, than, wood...   \n",
       "2331   2331  [plastic, laminate, flooring, is, one, of, the...   \n",
       "2332   2332  [plastic, has, long, been, considered, superio...   \n",
       "2333   2333  [in, the, content, of, search, results, ,, bin...   \n",
       "\n",
       "                                                   tags  \n",
       "0     [O, O, O, O, O, O, B-Object, O, O, O, B-Predic...  \n",
       "1     [O, O, O, O, O, B-Object, O, B-Predicate, O, B...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, B-Object, O, O, B-Predicate, O, B...  \n",
       "4     [O, O, B-Object, O, O, B-Predicate, O, O, O, B...  \n",
       "...                                                 ...  \n",
       "2329   [O, O, O, B-Object, B-Predicate, O, B-Object, O]  \n",
       "2330  [O, B-Object, O, O, B-Predicate, O, B-Object, ...  \n",
       "2331  [B-Object, O, B-Aspect, O, O, O, O, O, O, O, O...  \n",
       "2332  [B-Object, O, O, O, O, B-Predicate, O, B-Objec...  \n",
       "2333  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "\n",
       "[2334 rows x 3 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_data содержит много примеров которые не ловит train - можно попробовать сжульничать и подмешать dev\n",
    "# ner_data = pd.concat([ner_data, val_data])\n",
    "ner_data = ner_data.reset_index()\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "086a907a-fb8f-4dc5-b200-c8f759991863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t48512\t0.7943%\n",
      "B-Object\t6174\t0.1011%\n",
      "B-Predicate\t3109\t0.05091%\n",
      "B-Aspect\t2069\t0.03388%\n",
      "I-Aspect\t591\t0.009677%\n",
      "I-Predicate\t427\t0.006992%\n",
      "I-Object\t192\t0.003144%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_list = []\n",
    "for item in ner_data['tags']:\n",
    "    label_list.extend(item)\n",
    "\n",
    "c = Counter(label_list)\n",
    "for (name, num) in c.most_common(7):\n",
    "    print(\"{}\\t{}\\t{:.4}%\".format(name, num, num/sum(c.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "09dc2812-a18a-4e69-a153-cb2e8262e7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t6734\t0.8052%\n",
      "B-Object\t781\t0.09339%\n",
      "B-Predicate\t386\t0.04616%\n",
      "B-Aspect\t257\t0.03073%\n",
      "I-Object\t113\t0.01351%\n",
      "I-Aspect\t52\t0.006218%\n",
      "I-Predicate\t40\t0.004783%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_list = []\n",
    "for item in val_data['tags']:\n",
    "    label_list.extend(item)\n",
    "\n",
    "c = Counter(label_list)\n",
    "for (name, num) in c.most_common(7):\n",
    "    print(\"{}\\t{}\\t{:.4}%\".format(name, num, num/sum(c.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7eb6e970-033b-4ec8-9b6a-e2c4aee3c5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Object',\n",
       " 'I-Predicate',\n",
       " 'I-Object',\n",
       " 'B-Predicate',\n",
       " 'I-Aspect',\n",
       " 'B-Aspect']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = []\n",
    "for item in ner_data['tags']:\n",
    "    label_list.extend(item)\n",
    "label_list = list(set(label_list))\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09cd2511-8b5a-41ab-92ee-cc04478c91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "24d8548a-adb4-4af3-aeaf-87efd8a177f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>485</td>\n",
       "      <td>[i, also, preferred, the, psp, controls, to, t...</td>\n",
       "      <td>[O, O, O, O, B-Object, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>1524</td>\n",
       "      <td>[durability, solid, concrete, construction, en...</td>\n",
       "      <td>[O, O, B-Object, O, O, O, B-Predicate, O, B-As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>366</td>\n",
       "      <td>[bawalker, -, dude, you, know, ibm, has, maybe...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             tokens  \\\n",
       "485     485  [i, also, preferred, the, psp, controls, to, t...   \n",
       "1524   1524  [durability, solid, concrete, construction, en...   \n",
       "366     366  [bawalker, -, dude, you, know, ibm, has, maybe...   \n",
       "\n",
       "                                                   tags  \n",
       "485   [O, O, O, O, B-Object, O, O, O, O, O, O, O, O,...  \n",
       "1524  [O, O, B-Object, O, O, O, B-Predicate, O, B-As...  \n",
       "366   [O, O, O, O, O, B-Object, O, O, O, O, O, O, O,...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15863724-3da8-4422-bb68-b3c3c8a665fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 1867\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'tokens', 'tags', '__index_level_0__'],\n",
       "        num_rows: 467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset, DatasetDict\n",
    "ner_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "20379051-d421-40e4-982c-68034871125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Object', 2: 'I-Predicate', 3: 'I-Object', 4: 'B-Predicate', 5: 'I-Aspect', 6: 'B-Aspect'} {'O': 0, 'B-Object': 1, 'I-Predicate': 2, 'I-Object': 3, 'B-Predicate': 4, 'I-Aspect': 5, 'B-Aspect': 6}\n"
     ]
    }
   ],
   "source": [
    "id2label = dict(enumerate(label_list))\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "55f7ca6a-4225-42e0-8e85-37ad3a019cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:455: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForTokenClassification: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "from transformers import AutoTokenizer, BertForTokenClassification, DistilBertTokenizer, DistilBertModel, DistilBertForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "# model_checkpoint = 'dslim/bert-base-NER'\n",
    "# model_checkpoint = 'distilbert-base-uncased'\n",
    "# model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "# model_checkpoint = 'nsi319/distilbert-base-uncased-finetuned-app'\n",
    "# model_checkpoint = 'bert-base-uncased'\n",
    "# model_checkpoint = 'xlm-roberta-base'\n",
    "model_checkpoint = 'microsoft/deberta-v3-base'\n",
    "# model_checkpoint = 'microsoft/deberta-v3-large'\n",
    "\n",
    "\n",
    "result_name = model_checkpoint.replace(\"/\", \"-\")\n",
    "\n",
    "if model_checkpoint == \"bert-base-uncased\":\n",
    "    # configuration = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    # print(configuration)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                       num_labels=len(id2label),\n",
    "                                                       id2label=id2label,\n",
    "                                                       label2id=label2id,\n",
    "                                                       # config=configuration\n",
    "                                                      )\n",
    "    model.config.hidden_dropout_prob = 0.2\n",
    "    model.config.attention_probs_dropout_prob = 0.2\n",
    "    \n",
    "else:    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=False)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, \n",
    "                                                            num_labels=len(label_list),\n",
    "                                                            ignore_mismatched_sizes=True,\n",
    "                                                            id2label=id2label,\n",
    "                                                            label2id=label2id\n",
    "                                                            )\n",
    "    model.config.hidden_dropout_prob = 0.2\n",
    "    model.config.attention_probs_dropout_prob = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94d4b495-11e9-4aba-9b82-b9258acbe476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 2032, 289, 8405, 372, 1229, 322, 493, 354, 1847, 335, 4062, 264, 2894, 366, 2450, 366, 263, 459, 3909, 265, 2119, 323, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (0, 2), (0, 6), (0, 3), (0, 4), (0, 2), (0, 6), (0, 4), (0, 4), (0, 4), (0, 7), (0, 2), (0, 4), (0, 1), (0, 4), (0, 1), (0, 3), (0, 4), (0, 7), (0, 2), (0, 3), (0, 1), (0, 0)]}\n",
      "['[CLS]', '▁metal', '▁or', '▁cement', '▁may', '▁hold', '▁up', '▁better', '▁than', '▁wood', '▁when', '▁exposed', '▁to', '▁rain', '▁,', '▁wind', '▁,', '▁and', '▁high', '▁amounts', '▁of', '▁sun', '▁.', '[SEP]']\n",
      "_\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(ner_dataset['test'][\"tokens\"][10], is_split_into_words=True,  return_offsets_mapping=True, )\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "if model_checkpoint == 'xlm-roberta-base' or model_checkpoint == 'microsoft/deberta-v3-base' or model_checkpoint == 'microsoft/deberta-v3-large' :\n",
    "    PRE_WORD = '_'\n",
    "else:\n",
    "    PRE_WORD = '##'\n",
    "# SOS_TOKEN = \"[CLS]\"\n",
    "# EOS_TOKEN = \"[SEP]\"\n",
    "print(tokenized_input)\n",
    "print(tokens)\n",
    "print(PRE_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2055156-50a6-4ee5-852f-896c90fd2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "775b5394-9d43-459c-9ea7-1e1251f39710",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "# tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a0ed2c88-d3d4-4a18-acf7-452292f8f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=7,\n",
    "    warmup_steps=500, \n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb4a50ad-e0e8-4059-9957-0475271cda3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c6cf0ba0-609b-4c41-9d5c-cbd71295076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e7024101-90cf-44ca-9c3b-6f21973f4589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Aspect': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'Object': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'Predicate': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ner_dataset['train'][4]\n",
    "labels = example['tags']\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "894743cc-8137-4f9e-b738-b6cb4c60b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels, inputs = p.predictions, p.label_ids, p.inputs\n",
    "    predictions = np.argmax(p.predictions, axis=2)\n",
    "\n",
    "    # send only the first token of each word to the evaluation\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    for prediction, label, tokens in zip(predictions, labels, inputs):\n",
    "        true_predictions.append([])\n",
    "        true_labels.append([])\n",
    "        # print(prediction, label, tokens)\n",
    "        for (p, l, t) in zip(prediction, label, tokens):\n",
    "            # print(l, p, tokenizer.convert_ids_to_tokens(int(t)))\n",
    "            if l != -100 and not tokenizer.convert_ids_to_tokens(int(t)).startswith(PRE_WORD):\n",
    "                # print('append')\n",
    "                true_predictions[-1].append(label_list[p])\n",
    "                true_labels[-1].append(label_list[l])\n",
    "    # print(true_predictions, true_labels)\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1c9c83f-414d-4e92-bf0c-c7199c095dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разморозка\n",
    "# for layer in model.roberta.encoder.layer[:freeze_layer_count]:\n",
    "NUM_TO_FREEZE = 0\n",
    "\n",
    "if NUM_TO_FREEZE == 0:\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "else:\n",
    "    for layer in model.bert.encoder.layer[:NUM_TO_FREEZE]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    for layer in model.bert.encoder.layer[NUM_TO_FREEZE:]:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b8cb1ace-dcef-4d8a-89b4-220ead7a2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = TrainingArguments(\n",
    "    \"ner\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=200, \n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    include_inputs_for_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "84a752f8-b9c8-416a-94a4-e615969cb16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9cc6574f-7ba9-4c45-919b-14a9e49fdbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-beautiful-curie-0/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='351' max='351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [351/351 00:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.774306</td>\n",
       "      <td>0.886093</td>\n",
       "      <td>0.826436</td>\n",
       "      <td>0.933106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.163655</td>\n",
       "      <td>0.829684</td>\n",
       "      <td>0.903311</td>\n",
       "      <td>0.864933</td>\n",
       "      <td>0.946185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155860</td>\n",
       "      <td>0.865360</td>\n",
       "      <td>0.896689</td>\n",
       "      <td>0.880746</td>\n",
       "      <td>0.951849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=351, training_loss=0.3143968500642695, metrics={'train_runtime': 45.2582, 'train_samples_per_second': 123.757, 'train_steps_per_second': 7.756, 'total_flos': 156359765566554.0, 'train_loss': 0.3143968500642695, 'epoch': 3.0})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "42d4787a-b159-4109-9614-a39f88b4a14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1558600664138794,\n",
       " 'eval_precision': 0.8653600340860673,\n",
       " 'eval_recall': 0.8966887417218543,\n",
       " 'eval_f1': 0.8807458803122291,\n",
       " 'eval_accuracy': 0.9518493835388204,\n",
       " 'eval_runtime': 0.993,\n",
       " 'eval_samples_per_second': 470.296,\n",
       " 'eval_steps_per_second': 30.212,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cd4d9bcc-586f-4c24-8f8a-5a9d339fc6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f'task1/models/{result_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "630e3b40-9dc3-4786-8255-0b55f14bffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(f\"task1/models/{result_name}\").to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a921571d-6e31-4076-b102-39c08c75ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "if DEV:\n",
    "    test_data = read_dataset(\"task1/dev.tsv\")#, splitter=\"\\n\")\n",
    "else:\n",
    "    test_data = read_dataset(\"task1/test_no_answers.tsv\", splitter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f05f3ab6-5403-4019-8729-9b1673f9f0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'tried', 'windows', '8', 'and', 'it', \"'\", 's', 'lighter', 'than', 'windows', 'xp', '.']\n",
      "['O', 'O', 'O', 'B-Object', 'O', 'O', 'O', 'O', 'O', 'B-Predicate', 'O', 'B-Object', 'B-Object', 'O']\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence):\n",
    "    inputs = tokenizer(sentence,\n",
    "                        is_split_into_words=True, \n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "    # move to gpu\n",
    "    ids = inputs[\"input_ids\"].to('cuda:0')\n",
    "    mask = inputs[\"attention_mask\"].to('cuda:0')\n",
    "    # forward pass\n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [label_list[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "    prediction = []\n",
    "    # print(inputs[\"offset_mapping\"].squeeze().tolist())\n",
    "    for token_pred, mapping, logs in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist(), active_logits):\n",
    "        # print(torch.nn.functional.sigmoid(logs))\n",
    "        # only predictions on first word pieces are important\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            prediction.append(token_pred[1])\n",
    "        else:\n",
    "            continue\n",
    "    return prediction\n",
    "\n",
    "# sentence = ner_dataset['train'][15]['tokens'] #\"@HuggingFace is a company based in New York, but is also has employees working in Paris\"\n",
    "# print(sentence)\n",
    "# print(predict_sentence(sentence))\n",
    "\n",
    "print(test_data[2][0])\n",
    "print(predict_sentence(test_data[2][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a07189ef-3f5c-4c57-8a83-a52a0a69367a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 283/283 [00:05<00:00, 53.34it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"task1/{result_name}.tsv\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            prediction = predict_sentence(sentence[0])\n",
    "            # prediction = predict_w_keyword(sentence[0])            \n",
    "            for w,t in zip(sentence[0], prediction):\n",
    "                # print(w, '\\t', t)\n",
    "                f.write(w+'\\t'+t+'\\n')\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "66191476-1b8a-4da2-b846-67f019ff5d8e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct B->I\n",
      "correct I->B\n",
      "receiver I-Aspect O\n",
      "receiver B-Aspect O\n",
      "correct I->B\n",
      "poverty I-Aspect O\n",
      "poverty B-Aspect O\n",
      "correct I->B\n",
      "income I-Aspect O\n",
      "income B-Aspect O\n",
      "correct I->B\n",
      "of I-Aspect O\n",
      "of B-Aspect O\n",
      "correct I->B\n",
      "fast I-Predicate O\n",
      "fast B-Predicate O\n",
      "correct B->I\n",
      "correct I->B\n",
      "timber I-Aspect O\n",
      "timber B-Aspect O\n",
      "correct B->I\n",
      "correct I->B\n",
      "glass I-Object O\n",
      "glass B-Object O\n",
      "correct B->I\n",
      "correct I->B\n",
      "sale I-Aspect O\n",
      "sale B-Aspect O\n",
      "correct I->B\n",
      "apparel I-Aspect O\n",
      "apparel B-Aspect O\n",
      "correct B->I\n"
     ]
    }
   ],
   "source": [
    "# Корректируем BIO разметку на предсказании если модель выдает некорректно I вперед B и отдельно стоящую I в B\n",
    "def fix_bio_in_file(fn, fnout):\n",
    "    words = []\n",
    "    tags = []\n",
    "    prev_tags = []\n",
    "    with open(fn, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\"\\r\\n\").split(\"\\t\")\n",
    "            if len(line) > 1:\n",
    "                words.append(line[0])\n",
    "                tags.append(line[1])\n",
    "            else:\n",
    "                words.append(line[0])\n",
    "                tags.append('')\n",
    "\n",
    "    with open(fnout, \"w\") as f:\n",
    "        prev_tags = tags.copy()\n",
    "        prev_tags.insert(0, 'O')\n",
    "        # print(prev_tags)\n",
    "        for w, t, p in zip(words, tags, prev_tags):\n",
    "            # print(w,t,p)\n",
    "            if p.startswith('O') and t.startswith('I'):\n",
    "                print(\"correct I->B\")\n",
    "                print(w, t, p)\n",
    "                t = 'B' + t[1:]\n",
    "                print(w, t, p)\n",
    "            if t.startswith('B') and t == p:\n",
    "                print(\"correct B->I\")\n",
    "                t = 'I' + t[1:]\n",
    "            if w == '':\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write(w + '\\t'+ t + '\\n')\n",
    "\n",
    "fix_bio_in_file(f\"task1/{result_name}.tsv\", f\"task1/{result_name}-fix.tsv\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e2e5546f-9742-4f37-8ae7-0d25c58c5fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate_f1_partial import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7195c8cc-4c41-42e1-b08e-3a550a63073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('task1/dev.tsv', f'task1/{result_name}-fix.tsv', f'task1/{result_name}_report-no-fix.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "921458ad-8268-4988-8eb3-05aaccd946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "553865e2-7d03-4938-abc1-6fcc472860f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile(f'task1/{result_name}-fix.zip', 'w') as zipObj2:\n",
    "   # Add multiple files to the zip\n",
    "   zipObj2.write(f'task1/{result_name}-fix.tsv', arcname=f'{result_name}-fix.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bec13b-909d-4942-aa11-9118673dfc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd38aa0-7e33-4c59-8cdd-d9aa9fd5c6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df72e10b-373d-49b9-bf8e-f21ee2faae57",
   "metadata": {},
   "source": [
    "## 3.2. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "acdb9a1d-e877-450e-9a6b-0638c1c9b9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def create_pipeline(model_name):\n",
    "    return pipeline('fill-mask', model=model_name, device=0)\n",
    "\n",
    "predict_mask = create_pipeline(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef03925d-f5a5-4418-9f59-9a33f10c7d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', \"'\", 't', 'understand', 'why', 'they', \"'\", 'd', 'turn', 'down', 'the', 'mit', 'engineer', 'who', 'is', 'probably', 'much', 'smarter', 'then', 'the', 'harvard', 'poetry', 'major', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def augment_sent_3_token(s, tag):\n",
    "    bio = [i for i, s in enumerate(tag) if not s.startswith('O')]\n",
    "    sent = s.copy()\n",
    "    slen = len(sent)\n",
    "\n",
    "    ch = np.random.choice(bio, 3,  replace=False)\n",
    "    ch = sorted(ch)\n",
    "    # print(ch)\n",
    "    masked_words = []\n",
    "    for c in ch:\n",
    "        masked_words.append(sent[c])\n",
    "        sent[c] = '[MASK]'\n",
    "    # print(sent)\n",
    "    # print(masked_words)\n",
    "    result = predict_mask(\" \".join(sent))  \n",
    "\n",
    "    res_words = []\n",
    "    for idx, mask in enumerate(result):\n",
    "        p = []\n",
    "        for pos in mask:\n",
    "            if pos['score'] > 0.10:\n",
    "                # print(pos)\n",
    "                p.append(pos['token_str'])\n",
    "        res_words.append(p)\n",
    "        # print('---')\n",
    "    # print(res_words)\n",
    "    \n",
    "    ret = []\n",
    "    for pairs in itertools.product(res_words[0], res_words[1], res_words[2]):\n",
    "        for idx, w in enumerate(pairs):\n",
    "            sent[ch[idx]] = w\n",
    "            # print(w)\n",
    "        ret.append(sent.copy())\n",
    "        # print(' '.join(s))\n",
    "    return ret\n",
    "\n",
    "pos = 22\n",
    "augs = augment_sent_3_token(ner_data['tokens'][pos], ner_data['tags'][pos])\n",
    "print(ner_data['tokens'][pos],'\\n')\n",
    "for au in augs:\n",
    "    print(au)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba51f75e-8da4-43e3-afb7-b5071daf826f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'is', 'even', 'harder', 'to', 'get', 'off', 'the', 'waitlist', 'from', 'mit', 'than', 'at', 'harvard', 'or', 'stanford', '.']\n",
      "[['harvard', 'is', 'even', 'easier', 'for', 'get', 'off', 'the', 'waitlist', 'from', 'mit', 'then', 'at', 'yale', 'or', 'rutgers.']]\n"
     ]
    }
   ],
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# решил не использовать, так как часто аугментация получается невыровненная. подойдет для классификации, а не для NER\n",
    "def augment_sent_nlpaug(sent):\n",
    "    text = \" \".join(sent)\n",
    "    aug = naw.ContextualWordEmbsAug(\n",
    "        model_path='bert-base-uncased', action=\"substitute\")\n",
    "    augmented_text = aug.augment(text)\n",
    "    return [sent.split(\" \") for sent in augmented_text]\n",
    "    \n",
    "\n",
    "pos = 24\n",
    "augs = augment_sent_nlpaug(ner_data['tokens'][pos]) \n",
    "print(ner_data['tokens'][pos])\n",
    "print(augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6c4265b-0c6d-4ed7-8086-a067090d5d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'snowboarding', 'more', 'dangerous', 'than', 'bicycling', 'and', 'slightly', 'safer', 'than', 'swimming', '.'] \n",
      "\n",
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'hiking', 'more', 'dangerous', 'than', 'bicycling', 'and', 'slightly', 'safer', 'than', 'swimming', '.']\n",
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'snowboarding', 'less', 'dangerous', 'than', 'bicycling', 'and', 'slightly', 'safer', 'than', 'swimming', '.']\n",
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'snowboarding', 'more', 'dangerous', 'than', 'hiking', 'and', 'slightly', 'safer', 'than', 'swimming', '.']\n",
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'snowboarding', 'more', 'dangerous', 'than', 'bicycling', 'and', 'slightly', 'safer', 'than', 'cycling', '.']\n",
      "['this', 'fatality', 'rate', 'makes', 'skiing', 'and', 'snowboarding', 'more', 'dangerous', 'than', 'bicycling', 'and', 'slightly', 'safer', 'than', 'cycling', '.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def augment_sent(s, tag):\n",
    "    bio = [i for i, s in enumerate(tag) if not s.startswith('O')]\n",
    "    augmented = []\n",
    "    for mask_token in bio:\n",
    "        sent = s.copy()\n",
    "        masked_word = sent[mask_token] \n",
    "        sent[mask_token] = '[MASK]'\n",
    "        # print(masked_word)\n",
    "        result = predict_mask(\" \".join(sent))\n",
    "        for r in result:\n",
    "            if r['token_str'] != masked_word and r['score'] > 0.15:\n",
    "                # print(r)\n",
    "                sent[mask_token] = r['token_str'].replace(\"#\", \"\")\n",
    "                augmented.append(sent)\n",
    "    return augmented\n",
    "\n",
    "pos = 15\n",
    "augs = augment_sent(ner_data['tokens'][pos], ner_data['tags'][pos])\n",
    "print(ner_data['tokens'][pos],'\\n')\n",
    "for au in augs:\n",
    "    print(au)\n",
    "# for sent in ner_data['tokens']:\n",
    "#     print(sent, '\\n', augment_sent(sent), '\\n\\n')\n",
    "\n",
    "# predict_mask(\"men should wear shirt and [MASK] for tomorrow's event.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60b08c56-0d28-4b87-9be2-d66bf57b4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2334 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/transformers/pipelines/base.py:1073: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  UserWarning,\n",
      "100%|██████████| 2334/2334 [02:28<00:00, 15.70it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[also, ,, i, have, recently, discovered, advil...</td>\n",
       "      <td>[O, O, O, O, O, O, B-Object, O, O, O, B-Predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[i, have, always, heard, that, motrin, is, bet...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[when, i, was, a, figure, skater, i, injuried,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[in, a, way, ,, halloween, is, even, better, t...</td>\n",
       "      <td>[O, O, O, O, B-Object, O, O, B-Predicate, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[i, think, halloween, is, actually, safer, tha...</td>\n",
       "      <td>[O, O, B-Object, O, O, B-Predicate, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20394</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[in, the, content, of, search, engines, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20395</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[in, the, content, of, search, results, ,, it,...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20396</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20397</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20398</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[in, the, content, of, search, results, ,, bin...</td>\n",
       "      <td>[O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22733 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                             tokens  \\\n",
       "0        0.0  [also, ,, i, have, recently, discovered, advil...   \n",
       "1        1.0  [i, have, always, heard, that, motrin, is, bet...   \n",
       "2        2.0  [when, i, was, a, figure, skater, i, injuried,...   \n",
       "3        3.0  [in, a, way, ,, halloween, is, even, better, t...   \n",
       "4        4.0  [i, think, halloween, is, actually, safer, tha...   \n",
       "...      ...                                                ...   \n",
       "20394    NaN  [in, the, content, of, search, engines, ,, bin...   \n",
       "20395    NaN  [in, the, content, of, search, results, ,, it,...   \n",
       "20396    NaN  [in, the, content, of, search, results, ,, bin...   \n",
       "20397    NaN  [in, the, content, of, search, results, ,, bin...   \n",
       "20398    NaN  [in, the, content, of, search, results, ,, bin...   \n",
       "\n",
       "                                                    tags  \n",
       "0      [O, O, O, O, O, O, B-Object, O, O, O, B-Predic...  \n",
       "1      [O, O, O, O, O, B-Object, O, B-Predicate, O, B...  \n",
       "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3      [O, O, O, O, B-Object, O, O, B-Predicate, O, B...  \n",
       "4      [O, O, B-Object, O, O, B-Predicate, O, O, O, B...  \n",
       "...                                                  ...  \n",
       "20394  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "20395  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "20396  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "20397  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "20398  [O, O, O, O, B-Aspect, I-Aspect, O, B-Object, ...  \n",
       "\n",
       "[22733 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "training_data_2 = []\n",
    "t = tqdm(total = len(ner_data['tokens']))\n",
    "for idx, (sent, tags) in enumerate(zip(ner_data['tokens'], ner_data['tags'])):\n",
    "    aug_sents = augment_sent_3_token(sent, tags)\n",
    "    for i, aug_sent in enumerate(aug_sents):\n",
    "        training_data_2.append((aug_sent, tags))\n",
    "    aug_sents = augment_sent(sent, tags)\n",
    "    for i, aug_sent in enumerate(aug_sents):\n",
    "        training_data_2.append((aug_sent, tags))\n",
    "    # if idx > 10:\n",
    "    #     break\n",
    "    t.update(1)\n",
    "t.close()\n",
    "         \n",
    "ner_data_aug = pd.DataFrame(training_data_2, columns=['tokens', 'tags'])\n",
    "ner_data = pd.concat([ner_data, ner_data_aug])\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efe77841-6b17-4e7f-8015-1cbdfc9be949",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data['str'] = ner_data['tokens'].apply(lambda x: \" \".join(x))\n",
    "val_data['str'] = val_data['tokens'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "668fc507-3833-46dd-8209-1e9eb99c611e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19879, 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = ner_data.drop_duplicates(subset=['str'])\n",
    "ner_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bbf4f98-49ed-4d73-ab1c-dae8342e0dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data.reset_index().to_csv('task1/augmented.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38d64b37-df39-4069-980e-b0401e1dfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ner_data = pd.read_csv('task1/augmented.tsv', sep='\\t', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "611c8a55-3857-484a-b953-c57bf8d6b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# с этой точки можно вернуться и попробовать переобучить БЕРТ\n",
    "ner_data.tokens = ner_data.tokens.apply(eval)\n",
    "ner_data.tags = ner_data.tags.apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f0ecac8-1c27-47ab-a6c5-6969c40f3324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19879, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c442c18-c9f5-48e6-9f09-b401cbcbb748",
   "metadata": {},
   "source": [
    "## 3.3. Идея с усреднением вероятностей предсказания класса от 3-х моделей\n",
    "не взлетела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a0086004-d06c-4b47-9812-8b6b4bf775de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence_w_probs(sentence):\n",
    "    inputs = tokenizer(sentence,\n",
    "                        is_split_into_words=True, \n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\")\n",
    "    # move to gpu\n",
    "    ids = inputs[\"input_ids\"].to('cuda:0')\n",
    "    mask = inputs[\"attention_mask\"].to('cuda:0')\n",
    "    # forward pass\n",
    "    outputs = model(ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [label_list[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "    prediction = []\n",
    "    probs = []\n",
    "    # print(inputs[\"offset_mapping\"].squeeze().tolist())\n",
    "    for token_pred, mapping, logs in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist(), \n",
    "                                         nn.functional.sigmoid(active_logits).detach().cpu().numpy()):\n",
    "        # print(torch.nn.functional.sigmoid(logs))\n",
    "      #only predictions on first word pieces are important\n",
    "        if mapping[0] == 0 and mapping[1] != 0:\n",
    "            prediction.append(token_pred[1])\n",
    "            probs.append(logs)\n",
    "        else:\n",
    "            continue\n",
    "    return prediction, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2cae1196-8f5a-49c0-a0e1-848118eb8b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/283 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████| 283/283 [00:02<00:00, 130.12it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(f\"task1/{result_name}-logs.tsv\", \"w\") as f:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            # print(sentence[0])\n",
    "            prediction, probs = predict_sentence_w_probs(sentence[0])\n",
    "            for w,t,p in zip(sentence[0], prediction, probs):\n",
    "                # print(w, '\\t', t)\n",
    "                f.write(w+'\\t'+t+'\\t'+'\\t'.join(map(str, p)) +'\\n')\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68aecd-cb79-49b7-9443-41506ba4ffcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# идея взять и дообучиться на вероятностях от 3-х моделей. \n",
    "# не успел по времени \n",
    "logits_files = ['dslim-bert-base-NER-logs.tsv',\n",
    "                'cointegrated-rubert-tiny2-logs.tsv',\n",
    "                'bert-base-uncased-logs.tsv']\n",
    "\n",
    "match = []\n",
    "for lf in logits_files:\n",
    "    with open('task1/'+lf, 'r') as f:\n",
    "        match.append(f.readlines())\n",
    "\n",
    "with open('task1/mean-prob.tsv', 'w') as f:        \n",
    "    for e1, e2, e3 in zip(match[0], match[1], match[2]):\n",
    "        r1 = e1.strip('\\n').split(\"\\t\")\n",
    "        r2 = e2.strip('\\n').split(\"\\t\")\n",
    "        r3 = e3.strip('\\n').split(\"\\t\")\n",
    "        if len(r1) > 2:\n",
    "            score1 = r1[2:]\n",
    "            score2 = r2[2:]\n",
    "            score3 = r3[2:]\n",
    "            res_score = [np.mean([float(s1), float(s2), float(s3)]) for s1, s2, s3 in zip(score1, score2, score3)]\n",
    "            # print(r1[:2], id2label[np.argmax(res_score)])\n",
    "            f.write(r1[0]+'\\t'+id2label[np.argmax(res_score)]+'\\n')\n",
    "        else:\n",
    "            f.write('\\n')\n",
    "            pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59782b4a-1035-43aa-8c95-975202e2a77f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# не взлетела - нужно не усреднять выходы, а добавлять знания из других источников\n",
    "fix_bio_in_file(f\"task1/mean-prob.tsv\", f\"task1/mean-prob-fix.tsv\")       \n",
    "main('task1/dev.tsv', f'task1/mean-prob-fix.tsv', f'task1/mean-prob_report.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad703e-3e56-40da-a750-533749a90d98",
   "metadata": {},
   "source": [
    "## 3.4 Набросок идеи про голосование 5 моделей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "edb25a03-faaf-41a4-8812-454c94d87de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# набросок идеи, чтоб просто использовать результаты от 5 моделей, и использовать \n",
    "def read_res_file(fn):\n",
    "    words = []\n",
    "    tags = []\n",
    "    with open(fn, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip(\"\\r\\n\").split(\"\\t\")\n",
    "            if len(line) > 1:\n",
    "                words.append(line[0])\n",
    "                tags.append(line[1])\n",
    "            else:\n",
    "                words.append(line[0])\n",
    "                tags.append('')\n",
    "    return words, tags\n",
    "    \n",
    "    \n",
    "words, tags1 = read_res_file(\"task1/bert-base-uncased-fix.tsv\")\n",
    "words, tags2 = read_res_file(\"task1/nsi319-distilbert-base-uncased-finetuned-app-fix.tsv\")\n",
    "words, tags3 = read_res_file(\"task1/distilbert-base-uncased-fix.tsv\")\n",
    "words, tags4 = read_res_file(\"task1/dslim-bert-base-NER-fix.tsv\")\n",
    "words, tags5 = read_res_file(\"task1/xlm-roberta-base-fix.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7f20bcb9-9cdb-46a5-9201-a64a818f0c25",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"task1/majority-class.tsv\", \"w\") as f:\n",
    "    for w, t1, t2, t3, t4, t5 in zip(words, tags1, tags2, tags3, tags4, tags5):\n",
    "        if w != '':\n",
    "            x = [label2id[t1], label2id[t2], label2id[t3],label2id[t4],label2id[t5]]\n",
    "            # выбираем метку класса, который встречается чаще других\n",
    "            unique, counts = np.unique(x, return_counts=True)\n",
    "            a = [(u, c) for u, c in zip(unique, counts)]\n",
    "            a = sorted(a, key= lambda x: x[1])\n",
    "            token_win =  a[-1][0]\n",
    "            # print(w, token_win, label_list[token_win])\n",
    "            # token_win = max(label2id[t1], label2id[t2], label2id[t3],label2id[t4],label2id[t5],)\n",
    "            # print(w, t1, t2, t3, t4, t5, '-->', label_list[token_win])\n",
    "            f.write(w+\"\\t\"+label_list[token_win]+\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")\n",
    "            # print('empty line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465081c-4cc7-4a3f-9f3c-78518ed0b25d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# не взлетела - нужно не усреднять выходы, а добавлять знания из других источников\n",
    "fix_bio_in_file(f\"task1/majority-class.tsv\", f\"task1/majority-class-fix.tsv\")       \n",
    "main('task1/dev.tsv', f'task1/majority-class-fix.tsv', f'task1/majority-class_report.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee16a7-120b-4f78-926c-44d75e63b155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47dd7dbd-df74-448a-8a2a-76299a04b1d6",
   "metadata": {},
   "source": [
    "## 3.5. Использовать информацию о ключевых словах для доразметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e881a7f9-2743-4b3a-ae1e-347c8ebdf7b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting keybert\n",
      "  Using cached keybert-0.7.0-py3-none-any.whl\n",
      "Requirement already satisfied: nltk in /home/user/conda/lib/python3.7/site-packages (3.6.7)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in /home/user/conda/lib/python3.7/site-packages (from keybert) (1.0.2)\n",
      "Collecting sentence-transformers>=0.3.8\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/user/conda/lib/python3.7/site-packages (from keybert) (1.21.5)\n",
      "Collecting rich>=10.4.0\n",
      "  Using cached rich-13.3.4-py3-none-any.whl (238 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/user/conda/lib/python3.7/site-packages (from keybert) (1.13.1)\n",
      "Requirement already satisfied: transformers>=3.5.1 in /home/user/conda/lib/python3.7/site-packages (from keybert) (4.28.1)\n",
      "Collecting flair>=0.7\n",
      "  Using cached flair-0.12.2-py3-none-any.whl (373 kB)\n",
      "Requirement already satisfied: tqdm in /home/user/conda/lib/python3.7/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/user/conda/lib/python3.7/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: joblib in /home/user/conda/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in /home/user/conda/lib/python3.7/site-packages (from nltk) (8.0.3)\n",
      "Collecting mpld3==0.3\n",
      "  Using cached mpld3-0.3-py3-none-any.whl\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting segtok>=1.5.7\n",
      "  Using cached segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Collecting wikipedia-api\n",
      "  Using cached Wikipedia_API-0.5.8-py3-none-any.whl (13 kB)\n",
      "Collecting gensim>=3.8.0\n",
      "  Using cached gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Collecting ftfy\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (1.2.13)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Using cached bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
      "Collecting conllu>=4.0\n",
      "  Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
      "Collecting pytorch-revgrad\n",
      "  Using cached pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Collecting gdown==4.4.0\n",
      "  Using cached gdown-4.4.0-py3-none-any.whl\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: boto3 in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (1.20.38)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: tabulate in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (0.8.9)\n",
      "Collecting janome\n",
      "  Using cached Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (2.8.2)\n",
      "Collecting pptree\n",
      "  Using cached pptree-3.1-py3-none-any.whl\n",
      "Collecting transformer-smaller-training-vocab>=0.2.1\n",
      "  Using cached transformer_smaller_training_vocab-0.2.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in /home/user/conda/lib/python3.7/site-packages (from flair>=0.7->keybert) (0.13.4)\n",
      "Requirement already satisfied: requests[socks] in /home/user/conda/lib/python3.7/site-packages (from gdown==4.4.0->flair>=0.7->keybert) (2.27.1)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from gdown==4.4.0->flair>=0.7->keybert) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from gdown==4.4.0->flair>=0.7->keybert) (3.4.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/user/conda/lib/python3.7/site-packages (from gdown==4.4.0->flair>=0.7->keybert) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /home/user/conda/lib/python3.7/site-packages (from rich>=10.4.0->keybert) (4.0.1)\n",
      "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
      "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Using cached Pygments-2.15.1-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.2->keybert) (3.0.0)\n",
      "Requirement already satisfied: torchvision in /home/user/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.10.1+cu111)\n",
      "Requirement already satisfied: sentencepiece in /home/user/conda/lib/python3.7/site-packages (from sentence-transformers>=0.3.8->keybert) (0.1.96)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/user/conda/lib/python3.7/site-packages (from torch>=1.4.0->keybert) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/user/conda/lib/python3.7/site-packages (from torch>=1.4.0->keybert) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/user/conda/lib/python3.7/site-packages (from torch>=1.4.0->keybert) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/user/conda/lib/python3.7/site-packages (from torch>=1.4.0->keybert) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/jovyan/.imgenv-mystifying-yonath-0/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->keybert) (60.5.0)\n",
      "Requirement already satisfied: wheel in /home/jovyan/.imgenv-mystifying-yonath-0/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->keybert) (0.37.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from transformers>=3.5.1->keybert) (21.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/conda/lib/python3.7/site-packages (from transformers>=3.5.1->keybert) (4.10.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/user/conda/lib/python3.7/site-packages (from transformers>=3.5.1->keybert) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from transformers>=3.5.1->keybert) (6.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/user/conda/lib/python3.7/site-packages (from deprecated>=1.2.4->flair>=0.7->keybert) (1.13.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: cloudpickle in /home/user/conda/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair>=0.7->keybert) (2.0.0)\n",
      "Collecting py4j\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Collecting networkx>=2.2\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Requirement already satisfied: future in /home/user/conda/lib/python3.7/site-packages (from hyperopt>=0.2.7->flair>=0.7->keybert) (0.18.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair>=0.7->keybert) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair>=0.7->keybert) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair>=0.7->keybert) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair>=0.7->keybert) (9.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib>=2.2.3->flair>=0.7->keybert) (4.28.5)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.98-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: datasets<3.0.0,>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (2.11.0)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /home/user/conda/lib/python3.7/site-packages (from transformers>=3.5.1->keybert) (3.19.3)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/user/conda/lib/python3.7/site-packages (from boto3->flair>=0.7->keybert) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.38 in /home/user/conda/lib/python3.7/site-packages (from boto3->flair>=0.7->keybert) (1.23.38)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/user/conda/lib/python3.7/site-packages (from boto3->flair>=0.7->keybert) (0.10.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/user/conda/lib/python3.7/site-packages (from ftfy->flair>=0.7->keybert) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=3.5.1->keybert) (3.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]->gdown==4.4.0->flair>=0.7->keybert) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]->gdown==4.4.0->flair>=0.7->keybert) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]->gdown==4.4.0->flair>=0.7->keybert) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]->gdown==4.4.0->flair>=0.7->keybert) (2021.10.8)\n",
      "Collecting torch>=1.4.0\n",
      "  Using cached torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (0.3.6)\n",
      "Requirement already satisfied: responses<0.19 in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (0.18.0)\n",
      "Requirement already satisfied: pandas in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (3.2.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (2022.1.0)\n",
      "Requirement already satisfied: multiprocess in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (0.70.14)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/conda/lib/python3.7/site-packages (from datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (3.8.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown==4.4.0->flair>=0.7->keybert) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]->gdown==4.4.0->flair>=0.7->keybert) (1.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (1.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (0.13.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (5.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (1.7.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/conda/lib/python3.7/site-packages (from pandas->datasets<3.0.0,>=2.0.0->transformer-smaller-training-vocab>=0.2.1->flair>=0.7->keybert) (2021.3)\n",
      "Installing collected packages: torch, smart-open, mdurl, sentencepiece, pygments, py4j, networkx, markdown-it-py, gensim, wikipedia-api, transformer-smaller-training-vocab, sqlitedict, sentence-transformers, segtok, rich, pytorch-revgrad, pptree, mpld3, more-itertools, lxml, langdetect, janome, hyperopt, gdown, ftfy, conllu, bpemb, keybert, flair\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.96\n",
      "    Uninstalling sentencepiece-0.1.96:\n",
      "      Successfully uninstalled sentencepiece-0.1.96\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.7.1\n",
      "    Uninstalling gdown-4.7.1:\n",
      "      Successfully uninstalled gdown-4.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ruprompts 0.1.4 requires torch<2.0.0,>=1.10.0, but you have torch 1.9.1 which is incompatible.\u001b[0m\n",
      "Successfully installed bpemb-0.3.4 conllu-4.5.2 flair-0.12.2 ftfy-6.1.1 gdown-4.4.0 gensim-4.2.0 hyperopt-0.2.7 janome-0.4.2 keybert-0.7.0 langdetect-1.0.9 lxml-4.9.2 markdown-it-py-2.2.0 mdurl-0.1.2 more-itertools-9.1.0 mpld3-0.3 networkx-2.6.3 pptree-3.1 py4j-0.10.9.7 pygments-2.15.1 pytorch-revgrad-0.2.0 rich-13.3.4 segtok-1.5.11 sentence-transformers-2.2.2 sentencepiece-0.1.98 smart-open-6.3.0 sqlitedict-2.1.0 torch-1.9.1 transformer-smaller-training-vocab-0.2.3 wikipedia-api-0.5.8\n"
     ]
    }
   ],
   "source": [
    "!pip install keybert keybert[flair] nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da5eb342-15f1-4cc6-9e71-0fb355fcc88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "60b7225b-20fc-4cdb-a92e-7fe56f693246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "roberta = TransformerDocumentEmbeddings('roberta-base')\n",
    "kw_model = KeyBERT(model=roberta)\n",
    "\n",
    "def get_kw_vector(sent):\n",
    "    keywords = kw_model.extract_keywords(sent, keyphrase_ngram_range=(1, 2))\n",
    "    kw = [1 if len(k) > 0 and k[0][1] > 0.5 else 0 for k in keywords]\n",
    "    return kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5894f4b4-1833-4c72-abe2-057d384839e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>pos</th>\n",
       "      <th>kw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[also, ,, i, have, recently, discovered, advil...</td>\n",
       "      <td>[O, O, O, O, O, O, B-Object, O, O, O, B-Predic...</td>\n",
       "      <td>[RB, ,, NN, VB, RB, VB, NN, NN, VB, RB, JJ, CC...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[i, have, always, heard, that, motrin, is, bet...</td>\n",
       "      <td>[O, O, O, O, O, B-Object, O, B-Predicate, O, B...</td>\n",
       "      <td>[NN, VB, RB, VB, IN, NN, VB, JJ, IN, NN, IN, N...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[when, i, was, a, figure, skater, i, injuried,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[WR, NN, VB, DT, NN, NN, NN, VB, PR, NN, PD, D...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                             tokens  \\\n",
       "0      0  [also, ,, i, have, recently, discovered, advil...   \n",
       "1      1  [i, have, always, heard, that, motrin, is, bet...   \n",
       "2      2  [when, i, was, a, figure, skater, i, injuried,...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [O, O, O, O, O, O, B-Object, O, O, O, B-Predic...   \n",
       "1  [O, O, O, O, O, B-Object, O, B-Predicate, O, B...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [RB, ,, NN, VB, RB, VB, NN, NN, VB, RB, JJ, CC...   \n",
       "1  [NN, VB, RB, VB, IN, NN, VB, JJ, IN, NN, IN, N...   \n",
       "2  [WR, NN, VB, DT, NN, NN, NN, VB, PR, NN, PD, D...   \n",
       "\n",
       "                                                  kw  \n",
       "0  [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, ...  \n",
       "1  [0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, ...  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6b687079-dbb8-4514-b162-22f54a45817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data['pos'] = ner_data['tokens'].apply(lambda x: [tag[:2] for w, tag in nltk.pos_tag(x)])\n",
    "val_data['pos'] = val_data['tokens'].apply(lambda x: [tag[:2] for w, tag in nltk.pos_tag(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7a9a116a-3e9e-4aed-87a3-b56cdfed75fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data['kw'] = ner_data['tokens'].apply(lambda x: get_kw_vector(x))\n",
    "val_data['kw'] = val_data['tokens'].apply(lambda x: get_kw_vector(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d28c9623-c861-4872-a0cd-6208072deece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>windows</td>\n",
       "      <td>O</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>O</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>still</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>going</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>strong</td>\n",
       "      <td>O</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>even</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>though</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>DT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day</td>\n",
       "      <td>O</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>about</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>windows</td>\n",
       "      <td>B-Object</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>I-Object</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>microsoft</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>announced</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>it</td>\n",
       "      <td>O</td>\n",
       "      <td>PR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'</td>\n",
       "      <td>O</td>\n",
       "      <td>''</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>s</td>\n",
       "      <td>O</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>approaching</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>450</td>\n",
       "      <td>O</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>million</td>\n",
       "      <td>O</td>\n",
       "      <td>CD</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>copies</td>\n",
       "      <td>O</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>windows</td>\n",
       "      <td>O</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>O</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sold</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>thus</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>far</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "      <td>,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>windows</td>\n",
       "      <td>B-Object</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7</td>\n",
       "      <td>I-Object</td>\n",
       "      <td>CD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>consumer</td>\n",
       "      <td>B-Aspect</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>usage</td>\n",
       "      <td>I-Aspect</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>coming</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>greater</td>\n",
       "      <td>B-Predicate</td>\n",
       "      <td>JJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>than</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>windows</td>\n",
       "      <td>B-Object</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>xp</td>\n",
       "      <td>I-Object</td>\n",
       "      <td>NN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1   2  3\n",
       "0       windows            O  NN  1\n",
       "1             7            O  CD  0\n",
       "2            is            O  VB  0\n",
       "3         still            O  RB  0\n",
       "4         going            O  VB  1\n",
       "5        strong            O  JJ  1\n",
       "6          even            O  RB  0\n",
       "7        though            O  IN  0\n",
       "8           the            O  DT  0\n",
       "9           day            O  NN  1\n",
       "10          was            O  VB  0\n",
       "11        about            O  RB  0\n",
       "12      windows     B-Object  JJ  1\n",
       "13            8     I-Object  CD  0\n",
       "14            ,            O   ,  0\n",
       "15    microsoft            O  RB  1\n",
       "16    announced            O  VB  1\n",
       "17           it            O  PR  0\n",
       "18            '            O  ''  0\n",
       "19            s            O  JJ  0\n",
       "20  approaching            O  VB  1\n",
       "21          450            O  CD  1\n",
       "22      million            O  CD  1\n",
       "23       copies            O  NN  1\n",
       "24           of            O  IN  0\n",
       "25      windows            O  NN  1\n",
       "26            7            O  CD  0\n",
       "27         sold            O  VB  1\n",
       "28         thus            O  RB  0\n",
       "29          far            O  RB  1\n",
       "30            ,            O   ,  0\n",
       "31         with            O  IN  0\n",
       "32      windows     B-Object  NN  1\n",
       "33            7     I-Object  CD  0\n",
       "34     consumer     B-Aspect  NN  1\n",
       "35        usage     I-Aspect  NN  1\n",
       "36       coming            O  VB  1\n",
       "37           in            O  IN  0\n",
       "38      greater  B-Predicate  JJ  1\n",
       "39         than            O  IN  0\n",
       "40      windows     B-Object  NN  1\n",
       "41           xp     I-Object  NN  1\n",
       "42            .            O   .  0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = 1\n",
    "kw = get_kw_vector(val_data['tokens'][pos])\n",
    "tab = []\n",
    "for w, t, p, k in zip(val_data['tokens'][pos], val_data['tags'][pos], val_data['pos'][pos], kw):\n",
    "    tab.append((w, t, p, k))\n",
    "pd.DataFrame.from_records(tab)\n",
    "# вопросы к разметке dev.tsv -- \n",
    "# почему в части случаев windows то размечена как B-Object, то не размечена?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f110c-edf1-4864-9802-0f7993479fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5934cd69-be94-489a-ab0c-87ee43d50a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'WP', 1: 'WD', 2: \"''\", 3: 'RP', 4: 'MD', 5: 'PR', 6: '.', 7: 'LS', 8: 'EX', 9: 'SY', 10: 'CD', 11: 'WR', 12: ',', 13: '$', 14: 'NN', 15: 'TO', 16: 'JJ', 17: 'VB', 18: 'DT', 19: 'PD', 20: 'PO', 21: '(', 22: 'FW', 23: '#', 24: 'CC', 25: 'IN', 26: ')', 27: 'RB', 28: ':', 29: 'UH'} {'WP': 0, 'WD': 1, \"''\": 2, 'RP': 3, 'MD': 4, 'PR': 5, '.': 6, 'LS': 7, 'EX': 8, 'SY': 9, 'CD': 10, 'WR': 11, ',': 12, '$': 13, 'NN': 14, 'TO': 15, 'JJ': 16, 'VB': 17, 'DT': 18, 'PD': 19, 'PO': 20, '(': 21, 'FW': 22, '#': 23, 'CC': 24, 'IN': 25, ')': 26, 'RB': 27, ':': 28, 'UH': 29}\n"
     ]
    }
   ],
   "source": [
    "pos_list = []\n",
    "for item in ner_data['pos']:\n",
    "    pos_list.extend(item)\n",
    "id2pos = dict(enumerate(set(pos_list)))\n",
    "pos2id = {v: k for k, v in id2pos.items()}\n",
    "print(id2pos, pos2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "63271101-7bf3-4c0c-91b7-8cc6ef3ed271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d68c9723-87af-4f9d-9a1e-0377ec79114e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'tokens', 'tags', 'pos', 'kw', '__index_level_0__'],\n",
       "        num_rows: 1867\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'tokens', 'tags', 'pos', 'kw', '__index_level_0__'],\n",
       "        num_rows: 467\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "959835f4-488f-45d8-9498-a4a33e7ef0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# попытка добавить данные к нашему датасету\n",
    "def tokenize_and_align_labels_w_sources(examples, label_all_tokens=False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    pos = []\n",
    "    for p in examples[\"pos\"]:\n",
    "        pos.append([pos2id[i] for i in p])\n",
    "    tokenized_inputs[\"posid\"] = pos\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ec413d50-ffd0-4dcf-8bd5-6c77a4c15016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_dataset.map(tokenize_and_align_labels_w_sources, batched=True)\n",
    "# tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d8e669aa-519e-4822-9199-f86e99dddfc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`tokens` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_831/915721714.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/data/data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         )\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"label\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3033\u001b[0m                 \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m     def create_token_type_ids_from_sequences(\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepend_batch_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mconvert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;34mf\" features (`{key}` in this case) have excessive nesting (inputs type `list` where type `int` is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                     \u001b[0;34m\" expected).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                 ) from e\n\u001b[0m\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`tokens` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "### Вот отсюда у меня не получилось понять как работать в парадигме HuggingFace с несколькими источниками\n",
    "## последовательных данных. Было бы круто, если бы кто-то показал как это делать - более элегантно, чем\n",
    "## как добавить тут данные?\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer, padding=True)\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=16, \n",
    "collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['test'], batch_size=16, collate_fn=data_collator)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):          \n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c33b36-e9b2-4661-b86e-550485aaa83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "99454510-b17e-4fe6-844f-aeac7b2ef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5582d676-57a0-4272-9180-8e0de89b1fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.0239012241363525,\n",
       " 'eval_precision': 0.01285140562248996,\n",
       " 'eval_recall': 0.04944812362030905,\n",
       " 'eval_f1': 0.02040072859744991,\n",
       " 'eval_accuracy': 0.07514161946017994,\n",
       " 'eval_runtime': 0.7209,\n",
       " 'eval_samples_per_second': 647.765,\n",
       " 'eval_steps_per_second': 41.612}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "87b41f28-666d-468c-8746-b53641c52f23",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2065,  1996,  2091, 24388,  2063,  3681,  1013,  5894,  1013,\n",
      "          2723,  2024, 10612,  2007,  3886,  2023,  2003,  6082,  1998, 16269,\n",
      "          2084,  3796,  2030,  5509,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2348,  2045,  2024,  2053, 15764,  2529,  2913,  2055,  1996,\n",
      "          3896,  1997,  5948,  5572,  2005,  3096,  2740,  1010,  5572,  1011,\n",
      "          2665,  1010,  2317,  1010,  2030,  2304,  1010, 24689,  7959, 15833,\n",
      "          2030, 11703, 10354,  7959, 15833,  1011,  2003,  2467,  1037,  2488,\n",
      "          3601,  2084, 14904,  1010,  5909, 10869,  1010,  2030,  5699,  2100,\n",
      "          4157,  9530,  3597, 22014,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2374,  2003,  3020,  6337,  2084,  3455,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2144, 11990, 15802,  2015,  4995,  1005,  1056,  3227,  2641,\n",
      "          2836,  3765,  1010,  2216, 28699,  2015,  2024,  6211,  2000,  2424,\n",
      "          2084,  1996,  2168,  3616,  2005,  1037, 16509,  2030, 13154,  1010,\n",
      "          6174, 28574,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2057,  2018,  2019,  4522, 20231,  1997,  9262,  2029,\n",
      "          3594,  9621,  1010,  2059,  9262,  3454,  2052,  2145,  2022,  2195,\n",
      "          2335,  2936,  2084,  5662, 26743,  3454,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2002,  2030,  2016,  2515,  2025,  2066,  2317,  6501,\n",
      "          1010,  2130,  7967,  6501,  2003,  2488,  2084, 10869,  2138,  2009,\n",
      "          2038,  5250,  1010, 13853,  1010, 17663,  1040,  1998, 18044,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4811,  2018,  1996,  3382,  2000,  2717,  6820, 14890,  5514,\n",
      "          2138,  2027,  2020,  3760,  1998,  3621,  3805,  1997,  1996,  7774,\n",
      "          2084, 17714,  1998, 13938,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4419, 13495,  2116, 13350,  1010,  3005,  3616,  2763,  2024,\n",
      "          3618,  2084,  1996,  2193,  1997, 11992,  4846,  2011, 13229,  1998,\n",
      "          1996,  2060,  6125,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13523,  6844,  2232,  3608, 11350,  2003,  2126,  2488,  2084,\n",
      "          2049,  4257,  3334,  1010, 15594,  3771,  1010,  2062,  7221,  2389,\n",
      "          5542,  1010, 13523,  6844,  2232,  7852,  1010,  2021,  1996,  7852,\n",
      "          2003,  5079,  2062,  4876,  2000,  1996,  6209,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2021,  2009,  2389,  3401,  3672,  2072,  2177,  2056,  1010,\n",
      "          2256, 13338, 11297,  2003,  2172,  6020,  1006,  2000,  1996, 13338,\n",
      "          5509,  1007,  1999,  3408,  1997,  3465,  1998,  3997,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11297,  2003,  1037,  2488,  5724,  2084,  3536,  2138,  2009,\n",
      "         16180,  2936,  1010,  5942,  2172,  2625,  6032,  1010,  1998,  2038,\n",
      "          8491,  7258,  2005, 12883,  2000,  2444,  1999,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2200,  3376,  5404,  2295,  2009,  9375, 14626, 10954,  4847,\n",
      "          2004,  2009,  3632,  2091,  5744,  2121,  2084,  1037,  3221,  1997,\n",
      "          6501,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 12456,  6806,  1041, 15907,  1010,  9733,  1010,  1998, 20643,\n",
      "          2031,  2488,  6695,  2084,  7513,  1010,  2144,  2027,  2747,  2491,\n",
      "          1996,  2391,  1997,  5096, 12598,  1012,  5136,  2008,  4903,  2278,\n",
      "          1011,  2241,  6475,  3786,  1996,  6471,  2125, 18133,  2213,  1011,\n",
      "          2241,  6475,  2011,  4346,  2488, 25223,  1998,  2062,  2836,  1011,\n",
      "          2241,  3643,  2005,  1996,  4748, 16874, 17288,  1005,  1055,  7922,\n",
      "          1012,   102],\n",
      "        [  101,  2023,  3376,  5572,  2243,  2604,  5942,  2062,  6176,  9344,\n",
      "          2084,  6081,  7923,  1010,  2021,  2009,  5683,  2488,  2104,  1037,\n",
      "          5442,  1998,  2009,  1005,  1055,  6082,  2000,  5441,  2084,  2060,\n",
      "          3536,  7923,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  2095,  1010,  1996,  5542,  2007,  1996, 14751,  3336,\n",
      "          2787,  2000,  3288,  2014,  2000,  3113,  1996,  2155,  2006, 14414,\n",
      "          1010,  2738,  2084, 15060,  2030,  4234,  1010,  2138,  3604,  2003,\n",
      "          6082,  1998, 14836,  2015,  8491,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2035,  2216,  2399,  2600,  1010,  1045,  6449,  2008,  2035,\n",
      "          1996, 16568,  2399,  2024,  2632,  4140,  6082,  2084,  1996,  2208,\n",
      "         10841,  4783,  4617,  2021,  2009,  1005,  1055,  2138,  2027,  2215,\n",
      "          2000,  4009,  1999,  2512, 27911,  2015,  2004,  2092,  2004,  2149,\n",
      "          1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,    0, -100, -100,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    6,    0,    0,    1,    0,    1,    0,    0,    0,    6,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    6,    0,    0,    0,    0,    6,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, -100, -100,    0,    0, -100, -100, -100,    0,\n",
      "            0,    0,    0,    1,    0,    0,    6,    0,    0,    6,    0,    0,\n",
      "            0, -100,    6,    0, -100, -100,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    6,    0,    1,    2,    0,    6,    0, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    6,    0, -100,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100,    0,    1,    0,    2,    0,    0,    0,    0,\n",
      "            0,    0,    6,    0,    6,    0,    0, -100,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    6,    0,    0,    0,\n",
      "            0,    0,    6,    2,    0,    0,    0,    0,    0,    1,    0,    0,\n",
      "            6,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    6,    0,    0,\n",
      "            0,    6,    0,    1,    0,    6,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    6,    0,    0,    0,    0,    2, -100, -100,    1,    0,    0,\n",
      "            0,    1,    0,    0,    0,    0,    0,    0,    0,    6,    0,    6,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    6,    0,    0,    2,    0,    0,    0,    0,    0,    1,    0,\n",
      "            0,    0,    0,    2,    0,    0,    6,    0,    0,    0,    0,    0,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0, -100, -100,    0,    6,    0,    0,    1,    0,    0,    0,\n",
      "         -100,    0,    0, -100,    0,    0,    0, -100,    0,    0,    0, -100,\n",
      "         -100,    6,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0, -100, -100, -100, -100,    0,    0,    0,    0,    0,\n",
      "            6,    0,    0,    1,    0,    0,    0,    0,    6,    0,    0,    0,\n",
      "            0,    2,    0,    2,    0, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    6,    0,    0,    1,    0,    0,    6,    0,    0,    2,    1,\n",
      "            0,    2,    0,    1,    2,    0,    0,    0,    1,    2,    5,    5,\n",
      "            0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    6,    0,    0,    0, -100,    0,    0,    0,    0,\n",
      "            0,    0,    1, -100,    0,    0,    0,    0,    6,    0, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0, -100,    6, -100,    0,    6,    0,    0,    6,    0,    1,\n",
      "            2,    0,    6,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, -100,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100, -100,    0,    0,    0,\n",
      "            0, -100],\n",
      "        [-100,    0,    0,    6, -100,    0,    2,    1,    0,    2,    0,    6,\n",
      "            0,    0,    0,    0,    0,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    1,    0,    2,    0,    0,    6,    0,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    6,    0,    0,    0,    6,\n",
      "            0,    6,    0,    0,    2,    0,    1,    0,    2, -100,    1,    0,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    6,\n",
      "            2,    0,    0, -100,    1,    0,    0,    6, -100, -100,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "            0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100]])}\n",
      "{'input_ids': tensor([[  101,  3455,  3475,  1005,  1056,  2004,  2919,  2004, 14002,  1998,\n",
      "          3256, 10080,  2073,  6794,  5646,  5848,  1998,  7781,  1006,  2007,\n",
      "          6034, 13827,  1007,  1010,  2021,  2009,  1005,  1055,  4788,  2084,\n",
      "          3598,  1010,  4715,  1010,  3873,  1998,  2374,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2043,  1045,  2001,  1037,  3275, 18815,  1045,  1999,  9103,\n",
      "         11998,  2026, 15392,  2035,  1996,  2051,  1998,  1996,  4248,  4355,\n",
      "          2126,  2067,  2006,  2115,  2519,  2003,  3180, 21656,  1997,  9587,\n",
      "         18886,  2078,  1006,  5514,  3772,  2084,  4748, 14762,  1998,  2017,\n",
      "          2064,  2202,  2009,  2062,  2411,  1007,  3256,  1998,  4363,  2009,\n",
      "          2682,  2115,  5099,  1012,   102,     0,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  2222,  2298,  2046,  1996,  5416,  2072,  1013,\n",
      "          5163,  4357,  2008,  2323,  2022,  6082,  2138,  2005,  2070,  3114,\n",
      "          4419,  2003,  2062, 16875,  2084, 13229,  2030,  2151,  1997,  1996,\n",
      "         12440,  2015,  1012,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2197,  2733,  1010,  9733, 22416,  2049,  2047,  2678,  1011,\n",
      "         11058,  5080,  1996,  9733,  2543,  2694,  1006,  2030,  2543,  9189,\n",
      "          1010,  2004,  1996,  3830,  9631,  1007,  1010,  2029,  1996,  2194,\n",
      "          2758,  2052,  2022,  5514,  1998,  6082,  2000,  2224,  2084,  6637,\n",
      "          5733,  2066,  1996,  8224, 18546, 10526,  6207,  2694,  1998,  1996,\n",
      "         20996,  5283,  3688,  1012,   102,     0,     0,     0,     0],\n",
      "        [  101,  1996,  3066,  2000, 11506,  2002, 13668,  6582,  1011, 24100,\n",
      "          2007, 11985,  2097,  3443,  1996,  2117,  1011,  2922,  2974,  2578,\n",
      "         10802,  2369,  9980, 13058,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9085,  2962,  2810,  9442,  2084,  5509,  1998,  2097,  2025,\n",
      "          8579,  2058,  2051,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2070, 12455,  2089,  5060,  2008,  4811,  1051,  6633,  8592,\n",
      "          7711,  2024, 14092,  2000,  2087, 17714,  1997,  2714, 13528,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1999,  1996,  4180,  1997,  3945,  3463,  1010, 17620,  2003,\n",
      "          2025, 10862,  6020,  2000,  8224,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2017,  2228,  1996, 11067,  2229,  2012, 11742,  2052,  3275,\n",
      "          2041,  2008,  2043,  2017,  2562,  2635,  1017,  1011,  1019,  1003,\n",
      "          1997,  3465,  2041,  2006,  2019,  3296,  3978,  2011,  2478, 16269,\n",
      "          3033,  2030, 13845,  1996,  3737,  1010,  2044,  1037,  2261,  2086,\n",
      "          1996,  2482,  2003,  2053,  2488,  2084,  1996, 10231,  2008, 13938,\n",
      "          1013,  4811,  1013, 17714,  2001, 14107,  2041,  1012,   102],\n",
      "        [  101,  2445,  2008,  1996, 12637,  1997,  2267,  3455,  2058,  1996,\n",
      "          1040,  1011,  2223,  2024,  2747,  3618,  2084,  2267,  5443,  1012,\n",
      "          3576,  2223,  3598,  1010,  2009,  2052,  3497,  2812,  2130,  7046,\n",
      "          8311,  2000,  2131,  1996,  4187,  3742,  1997,  5848,  4072,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2096,  7513,  2038, 11807,  2049,  8069,  2006,  6020,  4800,\n",
      "          1011,  2865,  7047,  1998,  2049,  4367,  1011, 13851, 12631, 22471,\n",
      "          1010,  8412,  2038,  5359,  1037, 16269,  1010, 13554,  2121,  4031,\n",
      "          2007,  2062,  2543,  1011,  2373,  2104,  1996,  7415,  1012,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2256,  3325,  2038,  2042,  2008, 24665, 12502,  2015,  2038,\n",
      "          2488,  9262,  2490,  2084,  3781, 12083,  2100,  1998,  2008,  2311,\n",
      "          1999, 24665, 12502,  2015,  2003,  2172,  6082,  1998,  5514,  2084,\n",
      "          2311,  1999,  9262,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2092,  1010,  2006,  1996,  2028,  2192,  1010,  9160,  1017,\n",
      "          1998, 12202,  9475, 29602,  2140, 17319,  2031,  2488,  8389,  2084,\n",
      "          1996, 16568,  1010,  2926,  1999,  3408,  1997,  5813,  1010,  2004,\n",
      "          2027,  2490, 10751,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  9733,  2071,  2036, 15697,  1999,  2838,  5514,  2084,  6207,\n",
      "          2064, 10047, 17570,  1012,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2144,  6081,  2003,  1037,  4788, 22686,  1010,  2009,  2097,\n",
      "         24501, 21149,  8491, 13139,  1998,  2009,  2097,  2022, 27486,  2084,\n",
      "          3536,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3849,  2066,  2002,  1005,  1055,  5128, 22359,  2006,  1037,\n",
      "         10369,  1010,  8040,  9468,  2015,  3598,  2136,  2003,  2130,  4788,\n",
      "          2084,  3455,  2030,  2374,  1012,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    6,    1,    4,    4,    4,    4,    4,    6,    0,    0,    6,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100,\n",
      "            0,    0,    0,    0,    1,    0,    6,    0,    6,    0,    6,    0,\n",
      "            6,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    6, -100, -100,    0,    1,    4,    0,\n",
      "            6, -100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0, -100,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    6,    0,    1,    4,\n",
      "            0,    6,    0,    0,    0,    0,    0, -100,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    6,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    6,    0,    2,    0,    0,    0, -100,    0,    0,    0,    0,\n",
      "            0,    0, -100,    0,    0,    0,    0,    0,    0,    1,    0,    1,\n",
      "            0,    2,    0,    0,    0,    0,    0,    6,    2, -100,    6,    2,\n",
      "            0,    0,    6, -100,    0,    0, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    6, -100, -100,    3,    3,    0,    0,\n",
      "            0,    0,    0,    1,    4,    4,    0,    0,    0,    0,    6,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    6,    2,    1,    0,    6,    0,    0,    0,    0,    0,\n",
      "            0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    6,    0, -100,    2,    5,    0,\n",
      "            1,    0,    0,    6,    0,    0,    0,    0, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    2,    5,    0,    6,    0,    1,    0,\n",
      "            1,    0,    6,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0, -100,    0,    6,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    1,    2,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    1,    4,    0,    0,    0,\n",
      "            0,    6,    0,    6,    0,    6,    0,    0,    0,    0, -100],\n",
      "        [-100,    0,    0,    0,    2,    0,    0,    6,    0,    0,    0,    0,\n",
      "            0,    0,    0,    1,    0,    0,    0,    0,    0,    0,    6,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    6,    0,    0,    0,    0,    0,    1,    2,    5,    5,\n",
      "            0,    0,    0,    0,    0,    0,    2, -100,    0,    6,    0,    0,\n",
      "            0,    1,    0,    1, -100,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    6, -100, -100,    0,    1,    2,\n",
      "            5,    0,    6, -100, -100,    0,    0,    2,    0,    6, -100, -100,\n",
      "            0,    0,    1,    0,    1,    0,    2,    0,    6,    0, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    6,    0,    0,    6,\n",
      "            0, -100, -100,    0,    0,    2,    0,    0,    0,    6,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    6,    0,    0,    2,    0,    0,    1,    0,    6,    0,    0,\n",
      "         -100,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    6,    0,    0,    1,    2,    0,    0,    0,    2, -100,\n",
      "            1,    2,    0,    0,    0,    0,    1,    0,    6,    0, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, -100, -100,    6,    0,    0,    0,    1,    0,    6,    0,    6,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(trainer.get_train_dataloader()):\n",
    "    print(batch)\n",
    "        if idx > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9b3b22bd-451c-4870-a2c8-434c7785855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "\n",
    "class BERT_NER(nn.Module):\n",
    "\n",
    "    def __init__(self, model_name, tagset_size):\n",
    "        super(BERT_NER, self).__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bert = RobertaModel.from_pretrained(model_name)\n",
    "        self.lstm = nn.LSTM(input_size=1024, hidden_size=1024//2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(1024, tagset_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, pos_tags, labels=None):\n",
    "        x = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "            ).last_hidden_state\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "bert_ner_model = BERT_NER('bert-base-uncased', len(id2label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52498a06-a199-451d-8cd6-4159e340996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45717397-f263-4bd3-ac77-7a64044eb4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
